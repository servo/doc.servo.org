initSidebarItems({"fn":[["__breakpoint","Inserts a breakpoint instruction."],["__clrex","Removes the exclusive lock created by LDREX"],["__crc32b","CRC32 single round checksum for bytes (8 bits)."],["__crc32cb","CRC32-C single round checksum for bytes (8 bits)."],["__crc32ch","CRC32-C single round checksum for half words (16 bits)."],["__crc32cw","CRC32-C single round checksum for words (32 bits)."],["__crc32h","CRC32 single round checksum for half words (16 bits)."],["__crc32w","CRC32 single round checksum for words (32 bits)."],["__dbg","Generates a DBG instruction."],["__dmb","Generates a DMB (data memory barrier) instruction or equivalent CP15 instruction."],["__dsb","Generates a DSB (data synchronization barrier) instruction or equivalent CP15 instruction."],["__isb","Generates an ISB (instruction synchronization barrier) instruction or equivalent CP15 instruction."],["__ldrex","Executes an exclusive LDR instruction for 32 bit value."],["__ldrexb","Executes an exclusive LDR instruction for 8 bit value."],["__ldrexh","Executes an exclusive LDR instruction for 16 bit value."],["__nop","Generates an unspecified no-op instruction."],["__qadd","Signed saturating addition"],["__qadd16","Saturating two 16-bit integer additions"],["__qadd8","Saturating four 8-bit integer additions"],["__qasx","Returns the 16-bit signed saturated equivalent of"],["__qdbl","Insert a QADD instruction"],["__qsax","Returns the 16-bit signed saturated equivalent of"],["__qsub","Signed saturating subtraction"],["__qsub16","Saturating two 16-bit integer subtraction"],["__qsub8","Saturating two 8-bit integer subtraction"],["__rsr","Reads a 32-bit system register"],["__rsrp","Reads a system register containing an address"],["__sadd16","Returns the 16-bit signed saturated equivalent of"],["__sadd8","Returns the 8-bit signed saturated equivalent of"],["__sasx","Returns the 16-bit signed equivalent of"],["__sel","Select bytes from each operand according to APSR GE flags"],["__sev","Generates a SEV (send a global event) hint instruction."],["__sevl","Generates a send a local event hint instruction."],["__shadd16","Signed halving parallel halfword-wise addition."],["__shadd8","Signed halving parallel byte-wise addition."],["__shsub16","Signed halving parallel halfword-wise subtraction."],["__shsub8","Signed halving parallel byte-wise subtraction."],["__smlabb","Insert a SMLABB instruction"],["__smlabt","Insert a SMLABT instruction"],["__smlad","Dual 16-bit Signed Multiply with Addition of products and 32-bit accumulation."],["__smlatb","Insert a SMLATB instruction"],["__smlatt","Insert a SMLATT instruction"],["__smlawb","Insert a SMLAWB instruction"],["__smlawt","Insert a SMLAWT instruction"],["__smlsd","Dual 16-bit Signed Multiply with Subtraction  of products and 32-bit accumulation and overflow detection."],["__smuad","Signed Dual Multiply Add."],["__smuadx","Signed Dual Multiply Add Reversed."],["__smulbb","Insert a SMULBB instruction"],["__smulbt","Insert a SMULTB instruction"],["__smultb","Insert a SMULTB instruction"],["__smultt","Insert a SMULTT instruction"],["__smulwb","Insert a SMULWB instruction"],["__smulwt","Insert a SMULWT instruction"],["__smusd","Signed Dual Multiply Subtract."],["__smusdx","Signed Dual Multiply Subtract Reversed."],["__ssub8","Inserts a `SSUB8` instruction."],["__strex","Executes an exclusive STR instruction for 32 bit values"],["__strexb","Executes an exclusive STR instruction for 8 bit values"],["__usad8","Sum of 8-bit absolute differences."],["__usada8","Sum of 8-bit absolute differences and constant."],["__usub8","Inserts a `USUB8` instruction."],["__wfe","Generates a WFE (wait for event) hint instruction, or nothing."],["__wfi","Generates a WFI (wait for interrupt) hint instruction, or nothing."],["__wsr","Writes a 32-bit system register"],["__wsrp","Writes a system register containing an address"],["__yield","Generates a YIELD hint instruction."],["_clz_u16","Count Leading Zeros."],["_clz_u32","Count Leading Zeros."],["_clz_u8","Count Leading Zeros."],["_rbit_u32","Reverse the bit order."],["_rev_u16","Reverse the order of the bytes."],["_rev_u16","Reverse the order of the bytes."],["_rev_u32","Reverse the order of the bytes."],["_rev_u32","Reverse the order of the bytes."],["vaba_s16",""],["vaba_s32",""],["vaba_s8",""],["vaba_u16",""],["vaba_u32",""],["vaba_u8",""],["vabal_s16","Signed Absolute difference and Accumulate Long"],["vabal_s32","Signed Absolute difference and Accumulate Long"],["vabal_s8","Signed Absolute difference and Accumulate Long"],["vabal_u16","Unsigned Absolute difference and Accumulate Long"],["vabal_u32","Unsigned Absolute difference and Accumulate Long"],["vabal_u8","Unsigned Absolute difference and Accumulate Long"],["vabaq_s16",""],["vabaq_s32",""],["vabaq_s8",""],["vabaq_u16",""],["vabaq_u32",""],["vabaq_u8",""],["vabd_f32","Absolute difference between the arguments of Floating"],["vabd_s16","Absolute difference between the arguments"],["vabd_s32","Absolute difference between the arguments"],["vabd_s8","Absolute difference between the arguments"],["vabd_u16","Absolute difference between the arguments"],["vabd_u32","Absolute difference between the arguments"],["vabd_u8","Absolute difference between the arguments"],["vabdl_s16","Signed Absolute difference Long"],["vabdl_s32","Signed Absolute difference Long"],["vabdl_s8","Signed Absolute difference Long"],["vabdl_u16","Unsigned Absolute difference Long"],["vabdl_u32","Unsigned Absolute difference Long"],["vabdl_u8","Unsigned Absolute difference Long"],["vabdq_f32","Absolute difference between the arguments of Floating"],["vabdq_s16","Absolute difference between the arguments"],["vabdq_s32","Absolute difference between the arguments"],["vabdq_s8","Absolute difference between the arguments"],["vabdq_u16","Absolute difference between the arguments"],["vabdq_u32","Absolute difference between the arguments"],["vabdq_u8","Absolute difference between the arguments"],["vabs_f32","Floating-point absolute value"],["vabs_s16","Absolute value (wrapping)."],["vabs_s32","Absolute value (wrapping)."],["vabs_s8","Absolute value (wrapping)."],["vabsq_f32","Floating-point absolute value"],["vabsq_s16","Absolute value (wrapping)."],["vabsq_s32","Absolute value (wrapping)."],["vabsq_s8","Absolute value (wrapping)."],["vadd_f32","Vector add."],["vadd_p16","Bitwise exclusive OR"],["vadd_p64","Bitwise exclusive OR"],["vadd_p8","Bitwise exclusive OR"],["vadd_s16","Vector add."],["vadd_s32","Vector add."],["vadd_s8","Vector add."],["vadd_u16","Vector add."],["vadd_u32","Vector add."],["vadd_u8","Vector add."],["vaddhn_high_s16","Add returning High Narrow (high half)."],["vaddhn_high_s32","Add returning High Narrow (high half)."],["vaddhn_high_s64","Add returning High Narrow (high half)."],["vaddhn_high_u16","Add returning High Narrow (high half)."],["vaddhn_high_u32","Add returning High Narrow (high half)."],["vaddhn_high_u64","Add returning High Narrow (high half)."],["vaddhn_s16","Add returning High Narrow."],["vaddhn_s32","Add returning High Narrow."],["vaddhn_s64","Add returning High Narrow."],["vaddhn_u16","Add returning High Narrow."],["vaddhn_u32","Add returning High Narrow."],["vaddhn_u64","Add returning High Narrow."],["vaddl_high_s16","Signed Add Long (vector, high half)."],["vaddl_high_s32","Signed Add Long (vector, high half)."],["vaddl_high_s8","Signed Add Long (vector, high half)."],["vaddl_high_u16","Unsigned Add Long (vector, high half)."],["vaddl_high_u32","Unsigned Add Long (vector, high half)."],["vaddl_high_u8","Unsigned Add Long (vector, high half)."],["vaddl_s16","Signed Add Long (vector)."],["vaddl_s32","Signed Add Long (vector)."],["vaddl_s8","Signed Add Long (vector)."],["vaddl_u16","Unsigned Add Long (vector)."],["vaddl_u32","Unsigned Add Long (vector)."],["vaddl_u8","Unsigned Add Long (vector)."],["vaddq_f32","Vector add."],["vaddq_p128","Bitwise exclusive OR"],["vaddq_p16","Bitwise exclusive OR"],["vaddq_p64","Bitwise exclusive OR"],["vaddq_p8","Bitwise exclusive OR"],["vaddq_s16","Vector add."],["vaddq_s32","Vector add."],["vaddq_s64","Vector add."],["vaddq_s8","Vector add."],["vaddq_u16","Vector add."],["vaddq_u32","Vector add."],["vaddq_u64","Vector add."],["vaddq_u8","Vector add."],["vaddw_high_s16","Signed Add Wide (high half)."],["vaddw_high_s32","Signed Add Wide (high half)."],["vaddw_high_s8","Signed Add Wide (high half)."],["vaddw_high_u16","Unsigned Add Wide (high half)."],["vaddw_high_u32","Unsigned Add Wide (high half)."],["vaddw_high_u8","Unsigned Add Wide (high half)."],["vaddw_s16","Signed Add Wide."],["vaddw_s32","Signed Add Wide."],["vaddw_s8","Signed Add Wide."],["vaddw_u16","Unsigned Add Wide."],["vaddw_u32","Unsigned Add Wide."],["vaddw_u8","Unsigned Add Wide."],["vaesdq_u8","AES single round decryption."],["vaeseq_u8","AES single round encryption."],["vaesimcq_u8","AES inverse mix columns."],["vaesmcq_u8","AES mix columns."],["vand_s16","Vector bitwise and"],["vand_s32","Vector bitwise and"],["vand_s64","Vector bitwise and"],["vand_s8","Vector bitwise and"],["vand_u16","Vector bitwise and"],["vand_u32","Vector bitwise and"],["vand_u64","Vector bitwise and"],["vand_u8","Vector bitwise and"],["vandq_s16","Vector bitwise and"],["vandq_s32","Vector bitwise and"],["vandq_s64","Vector bitwise and"],["vandq_s8","Vector bitwise and"],["vandq_u16","Vector bitwise and"],["vandq_u32","Vector bitwise and"],["vandq_u64","Vector bitwise and"],["vandq_u8","Vector bitwise and"],["vbic_s16","Vector bitwise bit clear"],["vbic_s32","Vector bitwise bit clear"],["vbic_s64","Vector bitwise bit clear"],["vbic_s8","Vector bitwise bit clear"],["vbic_u16","Vector bitwise bit clear"],["vbic_u32","Vector bitwise bit clear"],["vbic_u64","Vector bitwise bit clear"],["vbic_u8","Vector bitwise bit clear"],["vbicq_s16","Vector bitwise bit clear"],["vbicq_s32","Vector bitwise bit clear"],["vbicq_s64","Vector bitwise bit clear"],["vbicq_s8","Vector bitwise bit clear"],["vbicq_u16","Vector bitwise bit clear"],["vbicq_u32","Vector bitwise bit clear"],["vbicq_u64","Vector bitwise bit clear"],["vbicq_u8","Vector bitwise bit clear"],["vbsl_f32","Bitwise Select."],["vbsl_p16","Bitwise Select."],["vbsl_p8","Bitwise Select."],["vbsl_s16","Bitwise Select."],["vbsl_s32","Bitwise Select."],["vbsl_s64","Bitwise Select."],["vbsl_s8","Bitwise Select instructions. This instruction sets each bit in the destination SIMD&FP register to the corresponding bit from the first source SIMD&FP register when the original destination bit was 1, otherwise from the second source SIMD&FP register. Bitwise Select."],["vbsl_u16","Bitwise Select."],["vbsl_u32","Bitwise Select."],["vbsl_u64","Bitwise Select."],["vbsl_u8","Bitwise Select."],["vbslq_f32","Bitwise Select. (128-bit)"],["vbslq_p16","Bitwise Select. (128-bit)"],["vbslq_p8","Bitwise Select. (128-bit)"],["vbslq_s16","Bitwise Select. (128-bit)"],["vbslq_s32","Bitwise Select. (128-bit)"],["vbslq_s64","Bitwise Select. (128-bit)"],["vbslq_s8","Bitwise Select. (128-bit)"],["vbslq_u16","Bitwise Select. (128-bit)"],["vbslq_u32","Bitwise Select. (128-bit)"],["vbslq_u64","Bitwise Select. (128-bit)"],["vbslq_u8","Bitwise Select. (128-bit)"],["vcage_f32","Floating-point absolute compare greater than or equal"],["vcageq_f32","Floating-point absolute compare greater than or equal"],["vcagt_f32","Floating-point absolute compare greater than"],["vcagtq_f32","Floating-point absolute compare greater than"],["vcale_f32","Floating-point absolute compare less than or equal"],["vcaleq_f32","Floating-point absolute compare less than or equal"],["vcalt_f32","Floating-point absolute compare less than"],["vcaltq_f32","Floating-point absolute compare less than"],["vceq_f32","Floating-point compare equal"],["vceq_p8","Compare bitwise Equal (vector)"],["vceq_s16","Compare bitwise Equal (vector)"],["vceq_s32","Compare bitwise Equal (vector)"],["vceq_s8","Compare bitwise Equal (vector)"],["vceq_u16","Compare bitwise Equal (vector)"],["vceq_u32","Compare bitwise Equal (vector)"],["vceq_u8","Compare bitwise Equal (vector)"],["vceqq_f32","Floating-point compare equal"],["vceqq_p8","Compare bitwise Equal (vector)"],["vceqq_s16","Compare bitwise Equal (vector)"],["vceqq_s32","Compare bitwise Equal (vector)"],["vceqq_s8","Compare bitwise Equal (vector)"],["vceqq_u16","Compare bitwise Equal (vector)"],["vceqq_u32","Compare bitwise Equal (vector)"],["vceqq_u8","Compare bitwise Equal (vector)"],["vcge_f32","Floating-point compare greater than or equal"],["vcge_s16","Compare signed greater than or equal"],["vcge_s32","Compare signed greater than or equal"],["vcge_s8","Compare signed greater than or equal"],["vcge_u16","Compare unsigned greater than or equal"],["vcge_u32","Compare unsigned greater than or equal"],["vcge_u8","Compare unsigned greater than or equal"],["vcgeq_f32","Floating-point compare greater than or equal"],["vcgeq_s16","Compare signed greater than or equal"],["vcgeq_s32","Compare signed greater than or equal"],["vcgeq_s8","Compare signed greater than or equal"],["vcgeq_u16","Compare unsigned greater than or equal"],["vcgeq_u32","Compare unsigned greater than or equal"],["vcgeq_u8","Compare unsigned greater than or equal"],["vcgt_f32","Floating-point compare greater than"],["vcgt_s16","Compare signed greater than"],["vcgt_s32","Compare signed greater than"],["vcgt_s8","Compare signed greater than"],["vcgt_u16","Compare unsigned highe"],["vcgt_u32","Compare unsigned highe"],["vcgt_u8","Compare unsigned highe"],["vcgtq_f32","Floating-point compare greater than"],["vcgtq_s16","Compare signed greater than"],["vcgtq_s32","Compare signed greater than"],["vcgtq_s8","Compare signed greater than"],["vcgtq_u16","Compare unsigned highe"],["vcgtq_u32","Compare unsigned highe"],["vcgtq_u8","Compare unsigned highe"],["vcle_f32","Floating-point compare less than or equal"],["vcle_s16","Compare signed less than or equal"],["vcle_s32","Compare signed less than or equal"],["vcle_s8","Compare signed less than or equal"],["vcle_u16","Compare unsigned less than or equal"],["vcle_u32","Compare unsigned less than or equal"],["vcle_u8","Compare unsigned less than or equal"],["vcleq_f32","Floating-point compare less than or equal"],["vcleq_s16","Compare signed less than or equal"],["vcleq_s32","Compare signed less than or equal"],["vcleq_s8","Compare signed less than or equal"],["vcleq_u16","Compare unsigned less than or equal"],["vcleq_u32","Compare unsigned less than or equal"],["vcleq_u8","Compare unsigned less than or equal"],["vcls_s16","Count leading sign bits"],["vcls_s32","Count leading sign bits"],["vcls_s8","Count leading sign bits"],["vcls_u16","Count leading sign bits"],["vcls_u32","Count leading sign bits"],["vcls_u8","Count leading sign bits"],["vclsq_s16","Count leading sign bits"],["vclsq_s32","Count leading sign bits"],["vclsq_s8","Count leading sign bits"],["vclsq_u16","Count leading sign bits"],["vclsq_u32","Count leading sign bits"],["vclsq_u8","Count leading sign bits"],["vclt_f32","Floating-point compare less than"],["vclt_s16","Compare signed less than"],["vclt_s32","Compare signed less than"],["vclt_s8","Compare signed less than"],["vclt_u16","Compare unsigned less than"],["vclt_u32","Compare unsigned less than"],["vclt_u8","Compare unsigned less than"],["vcltq_f32","Floating-point compare less than"],["vcltq_s16","Compare signed less than"],["vcltq_s32","Compare signed less than"],["vcltq_s8","Compare signed less than"],["vcltq_u16","Compare unsigned less than"],["vcltq_u32","Compare unsigned less than"],["vcltq_u8","Compare unsigned less than"],["vclz_s16","Count leading zero bits"],["vclz_s32","Count leading zero bits"],["vclz_s8","Count leading zero bits"],["vclz_u16","Count leading zero bits"],["vclz_u32","Count leading zero bits"],["vclz_u8","Count leading zero bits"],["vclzq_s16","Count leading zero bits"],["vclzq_s32","Count leading zero bits"],["vclzq_s8","Count leading zero bits"],["vclzq_u16","Count leading zero bits"],["vclzq_u32","Count leading zero bits"],["vclzq_u8","Count leading zero bits"],["vcnt_p8","Population count per byte."],["vcnt_s8","Population count per byte."],["vcnt_u8","Population count per byte."],["vcntq_p8","Population count per byte."],["vcntq_s8","Population count per byte."],["vcntq_u8","Population count per byte."],["vcreate_f32","Insert vector element from another vector element"],["vcreate_p16","Insert vector element from another vector element"],["vcreate_p64","Insert vector element from another vector element"],["vcreate_p8","Insert vector element from another vector element"],["vcreate_s16","Insert vector element from another vector element"],["vcreate_s32","Insert vector element from another vector element"],["vcreate_s64","Insert vector element from another vector element"],["vcreate_s8","Insert vector element from another vector element"],["vcreate_u16","Insert vector element from another vector element"],["vcreate_u32","Insert vector element from another vector element"],["vcreate_u64","Insert vector element from another vector element"],["vcreate_u8","Insert vector element from another vector element"],["vcvt_f32_s32","Fixed-point convert to floating-point"],["vcvt_f32_u32","Fixed-point convert to floating-point"],["vcvt_s32_f32","Floating-point convert to signed fixed-point, rounding toward zero"],["vcvt_u32_f32","Floating-point convert to unsigned fixed-point, rounding toward zero"],["vcvtq_f32_s32","Fixed-point convert to floating-point"],["vcvtq_f32_u32","Fixed-point convert to floating-point"],["vcvtq_s32_f32","Floating-point Convert to Signed fixed-point, rounding toward Zero (vector)"],["vcvtq_s32_f32","Floating-point convert to signed fixed-point, rounding toward zero"],["vcvtq_u32_f32","Floating-point Convert to Unsigned fixed-point, rounding toward Zero (vector)"],["vcvtq_u32_f32","Floating-point convert to unsigned fixed-point, rounding toward zero"],["vdup_lane_f32","Set all vector lanes to the same value"],["vdup_lane_p16","Set all vector lanes to the same value"],["vdup_lane_p8","Set all vector lanes to the same value"],["vdup_lane_s16","Set all vector lanes to the same value"],["vdup_lane_s32","Set all vector lanes to the same value"],["vdup_lane_s64","Set all vector lanes to the same value"],["vdup_lane_s8","Set all vector lanes to the same value"],["vdup_lane_u16","Set all vector lanes to the same value"],["vdup_lane_u32","Set all vector lanes to the same value"],["vdup_lane_u64","Set all vector lanes to the same value"],["vdup_lane_u8","Set all vector lanes to the same value"],["vdup_laneq_f32","Set all vector lanes to the same value"],["vdup_laneq_p16","Set all vector lanes to the same value"],["vdup_laneq_p8","Set all vector lanes to the same value"],["vdup_laneq_s16","Set all vector lanes to the same value"],["vdup_laneq_s32","Set all vector lanes to the same value"],["vdup_laneq_s64","Set all vector lanes to the same value"],["vdup_laneq_s8","Set all vector lanes to the same value"],["vdup_laneq_u16","Set all vector lanes to the same value"],["vdup_laneq_u32","Set all vector lanes to the same value"],["vdup_laneq_u64","Set all vector lanes to the same value"],["vdup_laneq_u8","Set all vector lanes to the same value"],["vdup_n_f32","Duplicate vector element to vector or scalar"],["vdup_n_p16","Duplicate vector element to vector or scalar"],["vdup_n_p8","Duplicate vector element to vector or scalar"],["vdup_n_s16","Duplicate vector element to vector or scalar"],["vdup_n_s32","Duplicate vector element to vector or scalar"],["vdup_n_s64","Duplicate vector element to vector or scalar"],["vdup_n_s8","Duplicate vector element to vector or scalar"],["vdup_n_u16","Duplicate vector element to vector or scalar"],["vdup_n_u32","Duplicate vector element to vector or scalar"],["vdup_n_u64","Duplicate vector element to vector or scalar"],["vdup_n_u8","Duplicate vector element to vector or scalar"],["vdupq_lane_f32","Set all vector lanes to the same value"],["vdupq_lane_p16","Set all vector lanes to the same value"],["vdupq_lane_p8","Set all vector lanes to the same value"],["vdupq_lane_s16","Set all vector lanes to the same value"],["vdupq_lane_s32","Set all vector lanes to the same value"],["vdupq_lane_s64","Set all vector lanes to the same value"],["vdupq_lane_s8","Set all vector lanes to the same value"],["vdupq_lane_u16","Set all vector lanes to the same value"],["vdupq_lane_u32","Set all vector lanes to the same value"],["vdupq_lane_u64","Set all vector lanes to the same value"],["vdupq_lane_u8","Set all vector lanes to the same value"],["vdupq_laneq_f32","Set all vector lanes to the same value"],["vdupq_laneq_p16","Set all vector lanes to the same value"],["vdupq_laneq_p8","Set all vector lanes to the same value"],["vdupq_laneq_s16","Set all vector lanes to the same value"],["vdupq_laneq_s32","Set all vector lanes to the same value"],["vdupq_laneq_s64","Set all vector lanes to the same value"],["vdupq_laneq_s8","Set all vector lanes to the same value"],["vdupq_laneq_u16","Set all vector lanes to the same value"],["vdupq_laneq_u32","Set all vector lanes to the same value"],["vdupq_laneq_u64","Set all vector lanes to the same value"],["vdupq_laneq_u8","Set all vector lanes to the same value"],["vdupq_n_f32","Duplicate vector element to vector or scalar"],["vdupq_n_p16","Duplicate vector element to vector or scalar"],["vdupq_n_p8","Duplicate vector element to vector or scalar"],["vdupq_n_s16","Duplicate vector element to vector or scalar"],["vdupq_n_s32","Duplicate vector element to vector or scalar"],["vdupq_n_s64","Duplicate vector element to vector or scalar"],["vdupq_n_s8","Duplicate vector element to vector or scalar"],["vdupq_n_u16","Duplicate vector element to vector or scalar"],["vdupq_n_u32","Duplicate vector element to vector or scalar"],["vdupq_n_u64","Duplicate vector element to vector or scalar"],["vdupq_n_u8","Duplicate vector element to vector or scalar"],["veor_s16","Vector bitwise exclusive or (vector)"],["veor_s32","Vector bitwise exclusive or (vector)"],["veor_s64","Vector bitwise exclusive or (vector)"],["veor_s8","Vector bitwise exclusive or (vector)"],["veor_u16","Vector bitwise exclusive or (vector)"],["veor_u32","Vector bitwise exclusive or (vector)"],["veor_u64","Vector bitwise exclusive or (vector)"],["veor_u8","Vector bitwise exclusive or (vector)"],["veorq_s16","Vector bitwise exclusive or (vector)"],["veorq_s32","Vector bitwise exclusive or (vector)"],["veorq_s64","Vector bitwise exclusive or (vector)"],["veorq_s8","Vector bitwise exclusive or (vector)"],["veorq_u16","Vector bitwise exclusive or (vector)"],["veorq_u32","Vector bitwise exclusive or (vector)"],["veorq_u64","Vector bitwise exclusive or (vector)"],["veorq_u8","Vector bitwise exclusive or (vector)"],["vext_f32","Extract vector from pair of vectors"],["vext_p16","Extract vector from pair of vectors"],["vext_p8","Extract vector from pair of vectors"],["vext_s16","Extract vector from pair of vectors"],["vext_s32","Extract vector from pair of vectors"],["vext_s64","Extract vector from pair of vectors"],["vext_s8","Extract vector from pair of vectors"],["vext_u16","Extract vector from pair of vectors"],["vext_u32","Extract vector from pair of vectors"],["vext_u64","Extract vector from pair of vectors"],["vext_u8","Extract vector from pair of vectors"],["vextq_f32","Extract vector from pair of vectors"],["vextq_p16","Extract vector from pair of vectors"],["vextq_p8","Extract vector from pair of vectors"],["vextq_s16","Extract vector from pair of vectors"],["vextq_s32","Extract vector from pair of vectors"],["vextq_s64","Extract vector from pair of vectors"],["vextq_s8","Extract vector from pair of vectors"],["vextq_u16","Extract vector from pair of vectors"],["vextq_u32","Extract vector from pair of vectors"],["vextq_u64","Extract vector from pair of vectors"],["vextq_u8","Extract vector from pair of vectors"],["vfma_f32","Floating-point fused Multiply-Add to accumulator(vector)"],["vfma_n_f32","Floating-point fused Multiply-Add to accumulator(vector)"],["vfmaq_f32","Floating-point fused Multiply-Add to accumulator(vector)"],["vfmaq_n_f32","Floating-point fused Multiply-Add to accumulator(vector)"],["vfms_f32","Floating-point fused multiply-subtract from accumulator"],["vfms_n_f32","Floating-point fused Multiply-subtract to accumulator(vector)"],["vfmsq_f32","Floating-point fused multiply-subtract from accumulator"],["vfmsq_n_f32","Floating-point fused Multiply-subtract to accumulator(vector)"],["vget_high_f32","Duplicate vector element to vector or scalar"],["vget_high_p16","Duplicate vector element to vector or scalar"],["vget_high_p8","Duplicate vector element to vector or scalar"],["vget_high_s16","Duplicate vector element to vector or scalar"],["vget_high_s32","Duplicate vector element to vector or scalar"],["vget_high_s64","Duplicate vector element to vector or scalar"],["vget_high_s8","Duplicate vector element to vector or scalar"],["vget_high_u16","Duplicate vector element to vector or scalar"],["vget_high_u32","Duplicate vector element to vector or scalar"],["vget_high_u64","Duplicate vector element to vector or scalar"],["vget_high_u8","Duplicate vector element to vector or scalar"],["vget_lane_f32","Duplicate vector element to vector or scalar"],["vget_lane_p16","Move vector element to general-purpose register"],["vget_lane_p64","Move vector element to general-purpose register"],["vget_lane_p8","Move vector element to general-purpose register"],["vget_lane_s16","Move vector element to general-purpose register"],["vget_lane_s32","Move vector element to general-purpose register"],["vget_lane_s64","Move vector element to general-purpose register"],["vget_lane_s8","Move vector element to general-purpose register"],["vget_lane_u16","Move vector element to general-purpose register"],["vget_lane_u32","Move vector element to general-purpose register"],["vget_lane_u64","Move vector element to general-purpose register"],["vget_lane_u8","Move vector element to general-purpose register"],["vget_low_f32","Duplicate vector element to vector or scalar"],["vget_low_p16","Duplicate vector element to vector or scalar"],["vget_low_p8","Duplicate vector element to vector or scalar"],["vget_low_s16","Duplicate vector element to vector or scalar"],["vget_low_s32","Duplicate vector element to vector or scalar"],["vget_low_s64","Duplicate vector element to vector or scalar"],["vget_low_s8","Duplicate vector element to vector or scalar"],["vget_low_u16","Duplicate vector element to vector or scalar"],["vget_low_u32","Duplicate vector element to vector or scalar"],["vget_low_u64","Duplicate vector element to vector or scalar"],["vget_low_u8","Duplicate vector element to vector or scalar"],["vgetq_lane_f32","Duplicate vector element to vector or scalar"],["vgetq_lane_p16","Move vector element to general-purpose register"],["vgetq_lane_p64","Move vector element to general-purpose register"],["vgetq_lane_p8","Move vector element to general-purpose register"],["vgetq_lane_s16","Move vector element to general-purpose register"],["vgetq_lane_s32","Move vector element to general-purpose register"],["vgetq_lane_s64","Move vector element to general-purpose register"],["vgetq_lane_s8","Move vector element to general-purpose register"],["vgetq_lane_u16","Move vector element to general-purpose register"],["vgetq_lane_u32","Move vector element to general-purpose register"],["vgetq_lane_u64","Move vector element to general-purpose register"],["vgetq_lane_u8","Move vector element to general-purpose register"],["vhadd_s16","Halving add"],["vhadd_s32","Halving add"],["vhadd_s8","Halving add"],["vhadd_u16","Halving add"],["vhadd_u32","Halving add"],["vhadd_u8","Halving add"],["vhaddq_s16","Halving add"],["vhaddq_s32","Halving add"],["vhaddq_s8","Halving add"],["vhaddq_u16","Halving add"],["vhaddq_u32","Halving add"],["vhaddq_u8","Halving add"],["vhsub_s16","Signed halving subtract"],["vhsub_s32","Signed halving subtract"],["vhsub_s8","Signed halving subtract"],["vhsub_u16","Signed halving subtract"],["vhsub_u32","Signed halving subtract"],["vhsub_u8","Signed halving subtract"],["vhsubq_s16","Signed halving subtract"],["vhsubq_s32","Signed halving subtract"],["vhsubq_s8","Signed halving subtract"],["vhsubq_u16","Signed halving subtract"],["vhsubq_u32","Signed halving subtract"],["vhsubq_u8","Signed halving subtract"],["vld1_dup_f32","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_p16","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_p64","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_p8","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_s16","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_s32","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_s64","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_s8","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_u16","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_u32","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_u64","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_dup_u8","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1_f32","Load multiple single-element structures to one, two, three, or four registers."],["vld1_f32_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_f32_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_f32_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_lane_f32","Load one single-element structure to one lane of one register."],["vld1_lane_p16","Load one single-element structure to one lane of one register."],["vld1_lane_p64","Load one single-element structure to one lane of one register."],["vld1_lane_p8","Load one single-element structure to one lane of one register."],["vld1_lane_s16","Load one single-element structure to one lane of one register."],["vld1_lane_s32","Load one single-element structure to one lane of one register."],["vld1_lane_s64","Load one single-element structure to one lane of one register."],["vld1_lane_s8","Load one single-element structure to one lane of one register."],["vld1_lane_u16","Load one single-element structure to one lane of one register."],["vld1_lane_u32","Load one single-element structure to one lane of one register."],["vld1_lane_u64","Load one single-element structure to one lane of one register."],["vld1_lane_u8","Load one single-element structure to one lane of one register."],["vld1_p16","Load multiple single-element structures to one, two, three, or four registers."],["vld1_p16_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_p16_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_p16_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_p64","Load multiple single-element structures to one, two, three, or four registers."],["vld1_p64_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_p64_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_p64_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_p8","Load multiple single-element structures to one, two, three, or four registers."],["vld1_p8_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_p8_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_p8_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s16","Load multiple single-element structures to one, two, three, or four registers."],["vld1_s16_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s16_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s16_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s32","Load multiple single-element structures to one, two, three, or four registers."],["vld1_s32_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s32_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s32_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s64","Load multiple single-element structures to one, two, three, or four registers."],["vld1_s64_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s64_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s64_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s8","Load multiple single-element structures to one, two, three, or four registers."],["vld1_s8_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s8_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_s8_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u16","Load multiple single-element structures to one, two, three, or four registers."],["vld1_u16_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u16_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u16_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u32","Load multiple single-element structures to one, two, three, or four registers."],["vld1_u32_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u32_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u32_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u64","Load multiple single-element structures to one, two, three, or four registers."],["vld1_u64_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u64_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u64_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u8","Load multiple single-element structures to one, two, three, or four registers."],["vld1_u8_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u8_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1_u8_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_dup_f32","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_p16","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_p64","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_p8","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_s16","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_s32","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_s64","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_s8","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_u16","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_u32","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_u64","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_dup_u8","Load one single-element structure and Replicate to all lanes (of one register)."],["vld1q_f32","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_f32_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_f32_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_f32_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_lane_f32","Load one single-element structure to one lane of one register."],["vld1q_lane_p16","Load one single-element structure to one lane of one register."],["vld1q_lane_p64","Load one single-element structure to one lane of one register."],["vld1q_lane_p8","Load one single-element structure to one lane of one register."],["vld1q_lane_s16","Load one single-element structure to one lane of one register."],["vld1q_lane_s32","Load one single-element structure to one lane of one register."],["vld1q_lane_s64","Load one single-element structure to one lane of one register."],["vld1q_lane_s8","Load one single-element structure to one lane of one register."],["vld1q_lane_u16","Load one single-element structure to one lane of one register."],["vld1q_lane_u32","Load one single-element structure to one lane of one register."],["vld1q_lane_u64","Load one single-element structure to one lane of one register."],["vld1q_lane_u8","Load one single-element structure to one lane of one register."],["vld1q_p16","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_p16_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_p16_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_p16_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_p64","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_p64_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_p64_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_p64_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_p8","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_p8_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_p8_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_p8_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s16","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_s16_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s16_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s16_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s32","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_s32_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s32_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s32_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s64","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_s64_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s64_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s64_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s8","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_s8_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s8_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_s8_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u16","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_u16_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u16_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u16_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u32","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_u32_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u32_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u32_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u64","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_u64_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u64_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u64_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u8","Load multiple single-element structures to one, two, three, or four registers."],["vld1q_u8_x2","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u8_x3","Load multiple single-element structures to one, two, three, or four registers"],["vld1q_u8_x4","Load multiple single-element structures to one, two, three, or four registers"],["vld2_dup_p16","Load single 2-element structure and replicate to all lanes of two registers"],["vld2_dup_p64","Load single 2-element structure and replicate to all lanes of two registers"],["vld2_dup_p8","Load single 2-element structure and replicate to all lanes of two registers"],["vld2_dup_u16","Load single 2-element structure and replicate to all lanes of two registers"],["vld2_dup_u32","Load single 2-element structure and replicate to all lanes of two registers"],["vld2_dup_u64","Load single 2-element structure and replicate to all lanes of two registers"],["vld2_dup_u8","Load single 2-element structure and replicate to all lanes of two registers"],["vld2_lane_p16","Load multiple 2-element structures to two registers"],["vld2_lane_p8","Load multiple 2-element structures to two registers"],["vld2_lane_u16","Load multiple 2-element structures to two registers"],["vld2_lane_u32","Load multiple 2-element structures to two registers"],["vld2_lane_u8","Load multiple 2-element structures to two registers"],["vld2_p16","Load multiple 2-element structures to two registers"],["vld2_p64","Load multiple 2-element structures to two registers"],["vld2_p8","Load multiple 2-element structures to two registers"],["vld2_u16","Load multiple 2-element structures to two registers"],["vld2_u32","Load multiple 2-element structures to two registers"],["vld2_u64","Load multiple 2-element structures to two registers"],["vld2_u8","Load multiple 2-element structures to two registers"],["vld2q_dup_p16","Load single 2-element structure and replicate to all lanes of two registers"],["vld2q_dup_p8","Load single 2-element structure and replicate to all lanes of two registers"],["vld2q_dup_u16","Load single 2-element structure and replicate to all lanes of two registers"],["vld2q_dup_u32","Load single 2-element structure and replicate to all lanes of two registers"],["vld2q_dup_u8","Load single 2-element structure and replicate to all lanes of two registers"],["vld2q_lane_p16","Load multiple 2-element structures to two registers"],["vld2q_lane_u16","Load multiple 2-element structures to two registers"],["vld2q_lane_u32","Load multiple 2-element structures to two registers"],["vld2q_p16","Load multiple 2-element structures to two registers"],["vld2q_p8","Load multiple 2-element structures to two registers"],["vld2q_u16","Load multiple 2-element structures to two registers"],["vld2q_u32","Load multiple 2-element structures to two registers"],["vld2q_u8","Load multiple 2-element structures to two registers"],["vld3_dup_p16","Load single 3-element structure and replicate to all lanes of three registers"],["vld3_dup_p64","Load single 3-element structure and replicate to all lanes of three registers"],["vld3_dup_p8","Load single 3-element structure and replicate to all lanes of three registers"],["vld3_dup_u16","Load single 3-element structure and replicate to all lanes of three registers"],["vld3_dup_u32","Load single 3-element structure and replicate to all lanes of three registers"],["vld3_dup_u64","Load single 3-element structure and replicate to all lanes of three registers"],["vld3_dup_u8","Load single 3-element structure and replicate to all lanes of three registers"],["vld3_lane_p16","Load multiple 3-element structures to three registers"],["vld3_lane_p8","Load multiple 3-element structures to three registers"],["vld3_lane_u16","Load multiple 3-element structures to three registers"],["vld3_lane_u32","Load multiple 3-element structures to three registers"],["vld3_lane_u8","Load multiple 3-element structures to three registers"],["vld3_p16","Load multiple 3-element structures to three registers"],["vld3_p64","Load multiple 3-element structures to three registers"],["vld3_p8","Load multiple 3-element structures to three registers"],["vld3_u16","Load multiple 3-element structures to three registers"],["vld3_u32","Load multiple 3-element structures to three registers"],["vld3_u64","Load multiple 3-element structures to three registers"],["vld3_u8","Load multiple 3-element structures to three registers"],["vld3q_dup_p16","Load single 3-element structure and replicate to all lanes of three registers"],["vld3q_dup_p8","Load single 3-element structure and replicate to all lanes of three registers"],["vld3q_dup_u16","Load single 3-element structure and replicate to all lanes of three registers"],["vld3q_dup_u32","Load single 3-element structure and replicate to all lanes of three registers"],["vld3q_dup_u8","Load single 3-element structure and replicate to all lanes of three registers"],["vld3q_lane_p16","Load multiple 3-element structures to three registers"],["vld3q_lane_u16","Load multiple 3-element structures to three registers"],["vld3q_lane_u32","Load multiple 3-element structures to three registers"],["vld3q_p16","Load multiple 3-element structures to three registers"],["vld3q_p8","Load multiple 3-element structures to three registers"],["vld3q_u16","Load multiple 3-element structures to three registers"],["vld3q_u32","Load multiple 3-element structures to three registers"],["vld3q_u8","Load multiple 3-element structures to three registers"],["vld4_dup_p16","Load single 4-element structure and replicate to all lanes of four registers"],["vld4_dup_p64","Load single 4-element structure and replicate to all lanes of four registers"],["vld4_dup_p8","Load single 4-element structure and replicate to all lanes of four registers"],["vld4_dup_u16","Load single 4-element structure and replicate to all lanes of four registers"],["vld4_dup_u32","Load single 4-element structure and replicate to all lanes of four registers"],["vld4_dup_u64","Load single 4-element structure and replicate to all lanes of four registers"],["vld4_dup_u8","Load single 4-element structure and replicate to all lanes of four registers"],["vld4_lane_p16","Load multiple 4-element structures to four registers"],["vld4_lane_p8","Load multiple 4-element structures to four registers"],["vld4_lane_u16","Load multiple 4-element structures to four registers"],["vld4_lane_u32","Load multiple 4-element structures to four registers"],["vld4_lane_u8","Load multiple 4-element structures to four registers"],["vld4_p16","Load multiple 4-element structures to four registers"],["vld4_p64","Load multiple 4-element structures to four registers"],["vld4_p8","Load multiple 4-element structures to four registers"],["vld4_u16","Load multiple 4-element structures to four registers"],["vld4_u32","Load multiple 4-element structures to four registers"],["vld4_u64","Load multiple 4-element structures to four registers"],["vld4_u8","Load multiple 4-element structures to four registers"],["vld4q_dup_p16","Load single 4-element structure and replicate to all lanes of four registers"],["vld4q_dup_p8","Load single 4-element structure and replicate to all lanes of four registers"],["vld4q_dup_u16","Load single 4-element structure and replicate to all lanes of four registers"],["vld4q_dup_u32","Load single 4-element structure and replicate to all lanes of four registers"],["vld4q_dup_u8","Load single 4-element structure and replicate to all lanes of four registers"],["vld4q_lane_p16","Load multiple 4-element structures to four registers"],["vld4q_lane_u16","Load multiple 4-element structures to four registers"],["vld4q_lane_u32","Load multiple 4-element structures to four registers"],["vld4q_p16","Load multiple 4-element structures to four registers"],["vld4q_p8","Load multiple 4-element structures to four registers"],["vld4q_u16","Load multiple 4-element structures to four registers"],["vld4q_u32","Load multiple 4-element structures to four registers"],["vld4q_u8","Load multiple 4-element structures to four registers"],["vldrq_p128","Load SIMD&FP register (immediate offset)"],["vmax_f32","Maximum (vector)"],["vmax_s16","Maximum (vector)"],["vmax_s32","Maximum (vector)"],["vmax_s8","Maximum (vector)"],["vmax_u16","Maximum (vector)"],["vmax_u32","Maximum (vector)"],["vmax_u8","Maximum (vector)"],["vmaxnm_f32","Floating-point Maximun Number (vector)"],["vmaxnmq_f32","Floating-point Maximun Number (vector)"],["vmaxq_f32","Maximum (vector)"],["vmaxq_s16","Maximum (vector)"],["vmaxq_s32","Maximum (vector)"],["vmaxq_s8","Maximum (vector)"],["vmaxq_u16","Maximum (vector)"],["vmaxq_u32","Maximum (vector)"],["vmaxq_u8","Maximum (vector)"],["vmin_f32","Minimum (vector)"],["vmin_s16","Minimum (vector)"],["vmin_s32","Minimum (vector)"],["vmin_s8","Minimum (vector)"],["vmin_u16","Minimum (vector)"],["vmin_u32","Minimum (vector)"],["vmin_u8","Minimum (vector)"],["vminnm_f32","Floating-point Minimun Number (vector)"],["vminnmq_f32","Floating-point Minimun Number (vector)"],["vminq_f32","Minimum (vector)"],["vminq_s16","Minimum (vector)"],["vminq_s32","Minimum (vector)"],["vminq_s8","Minimum (vector)"],["vminq_u16","Minimum (vector)"],["vminq_u32","Minimum (vector)"],["vminq_u8","Minimum (vector)"],["vmla_f32","Floating-point multiply-add to accumulator"],["vmla_lane_f32","Vector multiply accumulate with scalar"],["vmla_lane_s16","Vector multiply accumulate with scalar"],["vmla_lane_s32","Vector multiply accumulate with scalar"],["vmla_lane_u16","Vector multiply accumulate with scalar"],["vmla_lane_u32","Vector multiply accumulate with scalar"],["vmla_laneq_f32","Vector multiply accumulate with scalar"],["vmla_laneq_s16","Vector multiply accumulate with scalar"],["vmla_laneq_s32","Vector multiply accumulate with scalar"],["vmla_laneq_u16","Vector multiply accumulate with scalar"],["vmla_laneq_u32","Vector multiply accumulate with scalar"],["vmla_n_f32","Vector multiply accumulate with scalar"],["vmla_n_s16","Vector multiply accumulate with scalar"],["vmla_n_s32","Vector multiply accumulate with scalar"],["vmla_n_u16","Vector multiply accumulate with scalar"],["vmla_n_u32","Vector multiply accumulate with scalar"],["vmla_s16","Multiply-add to accumulator"],["vmla_s32","Multiply-add to accumulator"],["vmla_s8","Multiply-add to accumulator"],["vmla_u16","Multiply-add to accumulator"],["vmla_u32","Multiply-add to accumulator"],["vmla_u8","Multiply-add to accumulator"],["vmlal_lane_s16","Vector widening multiply accumulate with scalar"],["vmlal_lane_s32","Vector widening multiply accumulate with scalar"],["vmlal_lane_u16","Vector widening multiply accumulate with scalar"],["vmlal_lane_u32","Vector widening multiply accumulate with scalar"],["vmlal_laneq_s16","Vector widening multiply accumulate with scalar"],["vmlal_laneq_s32","Vector widening multiply accumulate with scalar"],["vmlal_laneq_u16","Vector widening multiply accumulate with scalar"],["vmlal_laneq_u32","Vector widening multiply accumulate with scalar"],["vmlal_n_s16","Vector widening multiply accumulate with scalar"],["vmlal_n_s32","Vector widening multiply accumulate with scalar"],["vmlal_n_u16","Vector widening multiply accumulate with scalar"],["vmlal_n_u32","Vector widening multiply accumulate with scalar"],["vmlal_s16","Signed multiply-add long"],["vmlal_s32","Signed multiply-add long"],["vmlal_s8","Signed multiply-add long"],["vmlal_u16","Unsigned multiply-add long"],["vmlal_u32","Unsigned multiply-add long"],["vmlal_u8","Unsigned multiply-add long"],["vmlaq_f32","Floating-point multiply-add to accumulator"],["vmlaq_lane_f32","Vector multiply accumulate with scalar"],["vmlaq_lane_s16","Vector multiply accumulate with scalar"],["vmlaq_lane_s32","Vector multiply accumulate with scalar"],["vmlaq_lane_u16","Vector multiply accumulate with scalar"],["vmlaq_lane_u32","Vector multiply accumulate with scalar"],["vmlaq_laneq_f32","Vector multiply accumulate with scalar"],["vmlaq_laneq_s16","Vector multiply accumulate with scalar"],["vmlaq_laneq_s32","Vector multiply accumulate with scalar"],["vmlaq_laneq_u16","Vector multiply accumulate with scalar"],["vmlaq_laneq_u32","Vector multiply accumulate with scalar"],["vmlaq_n_f32","Vector multiply accumulate with scalar"],["vmlaq_n_s16","Vector multiply accumulate with scalar"],["vmlaq_n_s32","Vector multiply accumulate with scalar"],["vmlaq_n_u16","Vector multiply accumulate with scalar"],["vmlaq_n_u32","Vector multiply accumulate with scalar"],["vmlaq_s16","Multiply-add to accumulator"],["vmlaq_s32","Multiply-add to accumulator"],["vmlaq_s8","Multiply-add to accumulator"],["vmlaq_u16","Multiply-add to accumulator"],["vmlaq_u32","Multiply-add to accumulator"],["vmlaq_u8","Multiply-add to accumulator"],["vmls_f32","Floating-point multiply-subtract from accumulator"],["vmls_lane_f32","Vector multiply subtract with scalar"],["vmls_lane_s16","Vector multiply subtract with scalar"],["vmls_lane_s32","Vector multiply subtract with scalar"],["vmls_lane_u16","Vector multiply subtract with scalar"],["vmls_lane_u32","Vector multiply subtract with scalar"],["vmls_laneq_f32","Vector multiply subtract with scalar"],["vmls_laneq_s16","Vector multiply subtract with scalar"],["vmls_laneq_s32","Vector multiply subtract with scalar"],["vmls_laneq_u16","Vector multiply subtract with scalar"],["vmls_laneq_u32","Vector multiply subtract with scalar"],["vmls_n_f32","Vector multiply subtract with scalar"],["vmls_n_s16","Vector multiply subtract with scalar"],["vmls_n_s32","Vector multiply subtract with scalar"],["vmls_n_u16","Vector multiply subtract with scalar"],["vmls_n_u32","Vector multiply subtract with scalar"],["vmls_s16","Multiply-subtract from accumulator"],["vmls_s32","Multiply-subtract from accumulator"],["vmls_s8","Multiply-subtract from accumulator"],["vmls_u16","Multiply-subtract from accumulator"],["vmls_u32","Multiply-subtract from accumulator"],["vmls_u8","Multiply-subtract from accumulator"],["vmlsl_lane_s16","Vector widening multiply subtract with scalar"],["vmlsl_lane_s32","Vector widening multiply subtract with scalar"],["vmlsl_lane_u16","Vector widening multiply subtract with scalar"],["vmlsl_lane_u32","Vector widening multiply subtract with scalar"],["vmlsl_laneq_s16","Vector widening multiply subtract with scalar"],["vmlsl_laneq_s32","Vector widening multiply subtract with scalar"],["vmlsl_laneq_u16","Vector widening multiply subtract with scalar"],["vmlsl_laneq_u32","Vector widening multiply subtract with scalar"],["vmlsl_n_s16","Vector widening multiply subtract with scalar"],["vmlsl_n_s32","Vector widening multiply subtract with scalar"],["vmlsl_n_u16","Vector widening multiply subtract with scalar"],["vmlsl_n_u32","Vector widening multiply subtract with scalar"],["vmlsl_s16","Signed multiply-subtract long"],["vmlsl_s32","Signed multiply-subtract long"],["vmlsl_s8","Signed multiply-subtract long"],["vmlsl_u16","Unsigned multiply-subtract long"],["vmlsl_u32","Unsigned multiply-subtract long"],["vmlsl_u8","Unsigned multiply-subtract long"],["vmlsq_f32","Floating-point multiply-subtract from accumulator"],["vmlsq_lane_f32","Vector multiply subtract with scalar"],["vmlsq_lane_s16","Vector multiply subtract with scalar"],["vmlsq_lane_s32","Vector multiply subtract with scalar"],["vmlsq_lane_u16","Vector multiply subtract with scalar"],["vmlsq_lane_u32","Vector multiply subtract with scalar"],["vmlsq_laneq_f32","Vector multiply subtract with scalar"],["vmlsq_laneq_s16","Vector multiply subtract with scalar"],["vmlsq_laneq_s32","Vector multiply subtract with scalar"],["vmlsq_laneq_u16","Vector multiply subtract with scalar"],["vmlsq_laneq_u32","Vector multiply subtract with scalar"],["vmlsq_n_f32","Vector multiply subtract with scalar"],["vmlsq_n_s16","Vector multiply subtract with scalar"],["vmlsq_n_s32","Vector multiply subtract with scalar"],["vmlsq_n_u16","Vector multiply subtract with scalar"],["vmlsq_n_u32","Vector multiply subtract with scalar"],["vmlsq_s16","Multiply-subtract from accumulator"],["vmlsq_s32","Multiply-subtract from accumulator"],["vmlsq_s8","Multiply-subtract from accumulator"],["vmlsq_u16","Multiply-subtract from accumulator"],["vmlsq_u32","Multiply-subtract from accumulator"],["vmlsq_u8","Multiply-subtract from accumulator"],["vmmlaq_s32","8-bit integer matrix multiply-accumulate"],["vmmlaq_u32","8-bit integer matrix multiply-accumulate"],["vmov_n_f32","Duplicate vector element to vector or scalar"],["vmov_n_p16","Duplicate vector element to vector or scalar"],["vmov_n_p8","Duplicate vector element to vector or scalar"],["vmov_n_s16","Duplicate vector element to vector or scalar"],["vmov_n_s32","Duplicate vector element to vector or scalar"],["vmov_n_s64","Duplicate vector element to vector or scalar"],["vmov_n_s8","Duplicate vector element to vector or scalar"],["vmov_n_u16","Duplicate vector element to vector or scalar"],["vmov_n_u32","Duplicate vector element to vector or scalar"],["vmov_n_u64","Duplicate vector element to vector or scalar"],["vmov_n_u8","Duplicate vector element to vector or scalar"],["vmovl_s16","Vector long move."],["vmovl_s32","Vector long move."],["vmovl_s8","Vector long move."],["vmovl_u16","Vector long move."],["vmovl_u32","Vector long move."],["vmovl_u8","Vector long move."],["vmovn_s16","Vector narrow integer."],["vmovn_s32","Vector narrow integer."],["vmovn_s64","Vector narrow integer."],["vmovn_u16","Vector narrow integer."],["vmovn_u32","Vector narrow integer."],["vmovn_u64","Vector narrow integer."],["vmovq_n_f32","Duplicate vector element to vector or scalar"],["vmovq_n_p16","Duplicate vector element to vector or scalar"],["vmovq_n_p8","Duplicate vector element to vector or scalar"],["vmovq_n_s16","Duplicate vector element to vector or scalar"],["vmovq_n_s32","Duplicate vector element to vector or scalar"],["vmovq_n_s64","Duplicate vector element to vector or scalar"],["vmovq_n_s8","Duplicate vector element to vector or scalar"],["vmovq_n_u16","Duplicate vector element to vector or scalar"],["vmovq_n_u32","Duplicate vector element to vector or scalar"],["vmovq_n_u64","Duplicate vector element to vector or scalar"],["vmovq_n_u8","Duplicate vector element to vector or scalar"],["vmul_f32","Multiply"],["vmul_lane_f32","Floating-point multiply"],["vmul_lane_s16","Multiply"],["vmul_lane_s32","Multiply"],["vmul_lane_u16","Multiply"],["vmul_lane_u32","Multiply"],["vmul_laneq_f32","Floating-point multiply"],["vmul_laneq_s16","Multiply"],["vmul_laneq_s32","Multiply"],["vmul_laneq_u16","Multiply"],["vmul_laneq_u32","Multiply"],["vmul_n_f32","Vector multiply by scalar"],["vmul_n_s16","Vector multiply by scalar"],["vmul_n_s32","Vector multiply by scalar"],["vmul_n_u16","Vector multiply by scalar"],["vmul_n_u32","Vector multiply by scalar"],["vmul_p8","Polynomial multiply"],["vmul_s16","Multiply"],["vmul_s32","Multiply"],["vmul_s8","Multiply"],["vmul_u16","Multiply"],["vmul_u32","Multiply"],["vmul_u8","Multiply"],["vmull_lane_s16","Vector long multiply by scalar"],["vmull_lane_s32","Vector long multiply by scalar"],["vmull_lane_u16","Vector long multiply by scalar"],["vmull_lane_u32","Vector long multiply by scalar"],["vmull_laneq_s16","Vector long multiply by scalar"],["vmull_laneq_s32","Vector long multiply by scalar"],["vmull_laneq_u16","Vector long multiply by scalar"],["vmull_laneq_u32","Vector long multiply by scalar"],["vmull_n_s16","Vector long multiply with scalar"],["vmull_n_s32","Vector long multiply with scalar"],["vmull_n_u16","Vector long multiply with scalar"],["vmull_n_u32","Vector long multiply with scalar"],["vmull_p8","Polynomial multiply long"],["vmull_s16","Signed multiply long"],["vmull_s32","Signed multiply long"],["vmull_s8","Signed multiply long"],["vmull_u16","Unsigned multiply long"],["vmull_u32","Unsigned multiply long"],["vmull_u8","Unsigned multiply long"],["vmulq_f32","Multiply"],["vmulq_lane_f32","Floating-point multiply"],["vmulq_lane_s16","Multiply"],["vmulq_lane_s32","Multiply"],["vmulq_lane_u16","Multiply"],["vmulq_lane_u32","Multiply"],["vmulq_laneq_f32","Floating-point multiply"],["vmulq_laneq_s16","Multiply"],["vmulq_laneq_s32","Multiply"],["vmulq_laneq_u16","Multiply"],["vmulq_laneq_u32","Multiply"],["vmulq_n_f32","Vector multiply by scalar"],["vmulq_n_s16","Vector multiply by scalar"],["vmulq_n_s32","Vector multiply by scalar"],["vmulq_n_u16","Vector multiply by scalar"],["vmulq_n_u32","Vector multiply by scalar"],["vmulq_p8","Polynomial multiply"],["vmulq_s16","Multiply"],["vmulq_s32","Multiply"],["vmulq_s8","Multiply"],["vmulq_u16","Multiply"],["vmulq_u32","Multiply"],["vmulq_u8","Multiply"],["vmvn_p8","Vector bitwise not."],["vmvn_s16","Vector bitwise not."],["vmvn_s32","Vector bitwise not."],["vmvn_s8","Vector bitwise not."],["vmvn_u16","Vector bitwise not."],["vmvn_u32","Vector bitwise not."],["vmvn_u8","Vector bitwise not."],["vmvnq_p8","Vector bitwise not."],["vmvnq_s16","Vector bitwise not."],["vmvnq_s32","Vector bitwise not."],["vmvnq_s8","Vector bitwise not."],["vmvnq_u16","Vector bitwise not."],["vmvnq_u32","Vector bitwise not."],["vmvnq_u8","Vector bitwise not."],["vneg_f32","Negate"],["vneg_s16","Negate"],["vneg_s32","Negate"],["vneg_s8","Negate"],["vnegq_f32","Negate"],["vnegq_s16","Negate"],["vnegq_s32","Negate"],["vnegq_s8","Negate"],["vorn_s16","Vector bitwise inclusive OR NOT"],["vorn_s32","Vector bitwise inclusive OR NOT"],["vorn_s64","Vector bitwise inclusive OR NOT"],["vorn_s8","Vector bitwise inclusive OR NOT"],["vorn_u16","Vector bitwise inclusive OR NOT"],["vorn_u32","Vector bitwise inclusive OR NOT"],["vorn_u64","Vector bitwise inclusive OR NOT"],["vorn_u8","Vector bitwise inclusive OR NOT"],["vornq_s16","Vector bitwise inclusive OR NOT"],["vornq_s32","Vector bitwise inclusive OR NOT"],["vornq_s64","Vector bitwise inclusive OR NOT"],["vornq_s8","Vector bitwise inclusive OR NOT"],["vornq_u16","Vector bitwise inclusive OR NOT"],["vornq_u32","Vector bitwise inclusive OR NOT"],["vornq_u64","Vector bitwise inclusive OR NOT"],["vornq_u8","Vector bitwise inclusive OR NOT"],["vorr_s16","Vector bitwise or (immediate, inclusive)"],["vorr_s32","Vector bitwise or (immediate, inclusive)"],["vorr_s64","Vector bitwise or (immediate, inclusive)"],["vorr_s8","Vector bitwise or (immediate, inclusive)"],["vorr_u16","Vector bitwise or (immediate, inclusive)"],["vorr_u32","Vector bitwise or (immediate, inclusive)"],["vorr_u64","Vector bitwise or (immediate, inclusive)"],["vorr_u8","Vector bitwise or (immediate, inclusive)"],["vorrq_s16","Vector bitwise or (immediate, inclusive)"],["vorrq_s32","Vector bitwise or (immediate, inclusive)"],["vorrq_s64","Vector bitwise or (immediate, inclusive)"],["vorrq_s8","Vector bitwise or (immediate, inclusive)"],["vorrq_u16","Vector bitwise or (immediate, inclusive)"],["vorrq_u32","Vector bitwise or (immediate, inclusive)"],["vorrq_u64","Vector bitwise or (immediate, inclusive)"],["vorrq_u8","Vector bitwise or (immediate, inclusive)"],["vpadal_s16","Signed Add and Accumulate Long Pairwise."],["vpadal_s32","Signed Add and Accumulate Long Pairwise."],["vpadal_s8","Signed Add and Accumulate Long Pairwise."],["vpadal_u16","Unsigned Add and Accumulate Long Pairwise."],["vpadal_u32","Unsigned Add and Accumulate Long Pairwise."],["vpadal_u8","Unsigned Add and Accumulate Long Pairwise."],["vpadalq_s16","Signed Add and Accumulate Long Pairwise."],["vpadalq_s32","Signed Add and Accumulate Long Pairwise."],["vpadalq_s8","Signed Add and Accumulate Long Pairwise."],["vpadalq_u16","Unsigned Add and Accumulate Long Pairwise."],["vpadalq_u32","Unsigned Add and Accumulate Long Pairwise."],["vpadalq_u8","Unsigned Add and Accumulate Long Pairwise."],["vpadd_f32","Floating-point add pairwise"],["vpadd_s16","Add pairwise."],["vpadd_s32","Add pairwise."],["vpadd_s8","Add pairwise."],["vpadd_u16","Add pairwise."],["vpadd_u32","Add pairwise."],["vpadd_u8","Add pairwise."],["vpaddl_s16","Signed Add Long Pairwise."],["vpaddl_s32","Signed Add Long Pairwise."],["vpaddl_s8","Signed Add Long Pairwise."],["vpaddl_u16","Unsigned Add Long Pairwise."],["vpaddl_u32","Unsigned Add Long Pairwise."],["vpaddl_u8","Unsigned Add Long Pairwise."],["vpaddlq_s16","Signed Add Long Pairwise."],["vpaddlq_s32","Signed Add Long Pairwise."],["vpaddlq_s8","Signed Add Long Pairwise."],["vpaddlq_u16","Unsigned Add Long Pairwise."],["vpaddlq_u32","Unsigned Add Long Pairwise."],["vpaddlq_u8","Unsigned Add Long Pairwise."],["vpmax_f32","Folding maximum of adjacent pairs"],["vpmax_s16","Folding maximum of adjacent pairs"],["vpmax_s32","Folding maximum of adjacent pairs"],["vpmax_s8","Folding maximum of adjacent pairs"],["vpmax_u16","Folding maximum of adjacent pairs"],["vpmax_u32","Folding maximum of adjacent pairs"],["vpmax_u8","Folding maximum of adjacent pairs"],["vpmin_f32","Folding minimum of adjacent pairs"],["vpmin_s16","Folding minimum of adjacent pairs"],["vpmin_s32","Folding minimum of adjacent pairs"],["vpmin_s8","Folding minimum of adjacent pairs"],["vpmin_u16","Folding minimum of adjacent pairs"],["vpmin_u32","Folding minimum of adjacent pairs"],["vpmin_u8","Folding minimum of adjacent pairs"],["vqabs_s16","Singned saturating Absolute value"],["vqabs_s32","Singned saturating Absolute value"],["vqabs_s8","Singned saturating Absolute value"],["vqabsq_s16","Singned saturating Absolute value"],["vqabsq_s32","Singned saturating Absolute value"],["vqabsq_s8","Singned saturating Absolute value"],["vqadd_s16","Saturating add"],["vqadd_s32","Saturating add"],["vqadd_s64","Saturating add"],["vqadd_s8","Saturating add"],["vqadd_u16","Saturating add"],["vqadd_u32","Saturating add"],["vqadd_u64","Saturating add"],["vqadd_u8","Saturating add"],["vqaddq_s16","Saturating add"],["vqaddq_s32","Saturating add"],["vqaddq_s64","Saturating add"],["vqaddq_s8","Saturating add"],["vqaddq_u16","Saturating add"],["vqaddq_u32","Saturating add"],["vqaddq_u64","Saturating add"],["vqaddq_u8","Saturating add"],["vqdmlal_lane_s16","Vector widening saturating doubling multiply accumulate with scalar"],["vqdmlal_lane_s32","Vector widening saturating doubling multiply accumulate with scalar"],["vqdmlal_n_s16","Vector widening saturating doubling multiply accumulate with scalar"],["vqdmlal_n_s32","Vector widening saturating doubling multiply accumulate with scalar"],["vqdmlal_s16","Signed saturating doubling multiply-add long"],["vqdmlal_s32","Signed saturating doubling multiply-add long"],["vqdmlsl_lane_s16","Vector widening saturating doubling multiply subtract with scalar"],["vqdmlsl_lane_s32","Vector widening saturating doubling multiply subtract with scalar"],["vqdmlsl_n_s16","Vector widening saturating doubling multiply subtract with scalar"],["vqdmlsl_n_s32","Vector widening saturating doubling multiply subtract with scalar"],["vqdmlsl_s16","Signed saturating doubling multiply-subtract long"],["vqdmlsl_s32","Signed saturating doubling multiply-subtract long"],["vqdmulh_laneq_s16","Vector saturating doubling multiply high by scalar"],["vqdmulh_laneq_s32","Vector saturating doubling multiply high by scalar"],["vqdmulh_n_s16","Vector saturating doubling multiply high with scalar"],["vqdmulh_n_s32","Vector saturating doubling multiply high with scalar"],["vqdmulh_s16","Signed saturating doubling multiply returning high half"],["vqdmulh_s32","Signed saturating doubling multiply returning high half"],["vqdmulhq_laneq_s16","Vector saturating doubling multiply high by scalar"],["vqdmulhq_laneq_s32","Vector saturating doubling multiply high by scalar"],["vqdmulhq_n_s16","Vector saturating doubling multiply high with scalar"],["vqdmulhq_n_s32","Vector saturating doubling multiply high with scalar"],["vqdmulhq_s16","Signed saturating doubling multiply returning high half"],["vqdmulhq_s32","Signed saturating doubling multiply returning high half"],["vqdmull_lane_s16","Vector saturating doubling long multiply by scalar"],["vqdmull_lane_s32","Vector saturating doubling long multiply by scalar"],["vqdmull_n_s16","Vector saturating doubling long multiply with scalar"],["vqdmull_n_s32","Vector saturating doubling long multiply with scalar"],["vqdmull_s16","Signed saturating doubling multiply long"],["vqdmull_s32","Signed saturating doubling multiply long"],["vqmovn_s16","Signed saturating extract narrow"],["vqmovn_s32","Signed saturating extract narrow"],["vqmovn_s64","Signed saturating extract narrow"],["vqmovn_u16","Unsigned saturating extract narrow"],["vqmovn_u32","Unsigned saturating extract narrow"],["vqmovn_u64","Unsigned saturating extract narrow"],["vqmovun_s16","Signed saturating extract unsigned narrow"],["vqmovun_s32","Signed saturating extract unsigned narrow"],["vqmovun_s64","Signed saturating extract unsigned narrow"],["vqneg_s16","Signed saturating negate"],["vqneg_s32","Signed saturating negate"],["vqneg_s8","Signed saturating negate"],["vqnegq_s16","Signed saturating negate"],["vqnegq_s32","Signed saturating negate"],["vqnegq_s8","Signed saturating negate"],["vqrdmlsh_lane_s16","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlsh_lane_s32","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlsh_laneq_s16","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlsh_laneq_s32","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlsh_s16","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlsh_s32","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlshq_lane_s16","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlshq_lane_s32","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlshq_laneq_s16","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlshq_laneq_s32","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlshq_s16","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmlshq_s32","Signed saturating rounding doubling multiply subtract returning high half"],["vqrdmulh_lane_s16","Vector rounding saturating doubling multiply high by scalar"],["vqrdmulh_lane_s32","Vector rounding saturating doubling multiply high by scalar"],["vqrdmulh_laneq_s16","Vector rounding saturating doubling multiply high by scalar"],["vqrdmulh_laneq_s32","Vector rounding saturating doubling multiply high by scalar"],["vqrdmulh_n_s16","Vector saturating rounding doubling multiply high with scalar"],["vqrdmulh_n_s32","Vector saturating rounding doubling multiply high with scalar"],["vqrdmulh_s16","Signed saturating rounding doubling multiply returning high half"],["vqrdmulh_s32","Signed saturating rounding doubling multiply returning high half"],["vqrdmulhq_lane_s16","Vector rounding saturating doubling multiply high by scalar"],["vqrdmulhq_lane_s32","Vector rounding saturating doubling multiply high by scalar"],["vqrdmulhq_laneq_s16","Vector rounding saturating doubling multiply high by scalar"],["vqrdmulhq_laneq_s32","Vector rounding saturating doubling multiply high by scalar"],["vqrdmulhq_n_s16","Vector saturating rounding doubling multiply high with scalar"],["vqrdmulhq_n_s32","Vector saturating rounding doubling multiply high with scalar"],["vqrdmulhq_s16","Signed saturating rounding doubling multiply returning high half"],["vqrdmulhq_s32","Signed saturating rounding doubling multiply returning high half"],["vqrshl_s16","Signed saturating rounding shift left"],["vqrshl_s32","Signed saturating rounding shift left"],["vqrshl_s64","Signed saturating rounding shift left"],["vqrshl_s8","Signed saturating rounding shift left"],["vqrshl_u16","Unsigned signed saturating rounding shift left"],["vqrshl_u32","Unsigned signed saturating rounding shift left"],["vqrshl_u64","Unsigned signed saturating rounding shift left"],["vqrshl_u8","Unsigned signed saturating rounding shift left"],["vqrshlq_s16","Signed saturating rounding shift left"],["vqrshlq_s32","Signed saturating rounding shift left"],["vqrshlq_s64","Signed saturating rounding shift left"],["vqrshlq_s8","Signed saturating rounding shift left"],["vqrshlq_u16","Unsigned signed saturating rounding shift left"],["vqrshlq_u32","Unsigned signed saturating rounding shift left"],["vqrshlq_u64","Unsigned signed saturating rounding shift left"],["vqrshlq_u8","Unsigned signed saturating rounding shift left"],["vqshl_n_s16","Signed saturating shift left"],["vqshl_n_s32","Signed saturating shift left"],["vqshl_n_s64","Signed saturating shift left"],["vqshl_n_s8","Signed saturating shift left"],["vqshl_n_u16","Unsigned saturating shift left"],["vqshl_n_u32","Unsigned saturating shift left"],["vqshl_n_u64","Unsigned saturating shift left"],["vqshl_n_u8","Unsigned saturating shift left"],["vqshl_s16","Signed saturating shift left"],["vqshl_s32","Signed saturating shift left"],["vqshl_s64","Signed saturating shift left"],["vqshl_s8","Signed saturating shift left"],["vqshl_u16","Unsigned saturating shift left"],["vqshl_u32","Unsigned saturating shift left"],["vqshl_u64","Unsigned saturating shift left"],["vqshl_u8","Unsigned saturating shift left"],["vqshlq_n_s16","Signed saturating shift left"],["vqshlq_n_s32","Signed saturating shift left"],["vqshlq_n_s64","Signed saturating shift left"],["vqshlq_n_s8","Signed saturating shift left"],["vqshlq_n_u16","Unsigned saturating shift left"],["vqshlq_n_u32","Unsigned saturating shift left"],["vqshlq_n_u64","Unsigned saturating shift left"],["vqshlq_n_u8","Unsigned saturating shift left"],["vqshlq_s16","Signed saturating shift left"],["vqshlq_s32","Signed saturating shift left"],["vqshlq_s64","Signed saturating shift left"],["vqshlq_s8","Signed saturating shift left"],["vqshlq_u16","Unsigned saturating shift left"],["vqshlq_u32","Unsigned saturating shift left"],["vqshlq_u64","Unsigned saturating shift left"],["vqshlq_u8","Unsigned saturating shift left"],["vqsub_s16","Saturating subtract"],["vqsub_s32","Saturating subtract"],["vqsub_s64","Saturating subtract"],["vqsub_s8","Saturating subtract"],["vqsub_u16","Saturating subtract"],["vqsub_u32","Saturating subtract"],["vqsub_u64","Saturating subtract"],["vqsub_u8","Saturating subtract"],["vqsubq_s16","Saturating subtract"],["vqsubq_s32","Saturating subtract"],["vqsubq_s64","Saturating subtract"],["vqsubq_s8","Saturating subtract"],["vqsubq_u16","Saturating subtract"],["vqsubq_u32","Saturating subtract"],["vqsubq_u64","Saturating subtract"],["vqsubq_u8","Saturating subtract"],["vraddhn_high_s16","Rounding Add returning High Narrow (high half)."],["vraddhn_high_s32","Rounding Add returning High Narrow (high half)."],["vraddhn_high_s64","Rounding Add returning High Narrow (high half)."],["vraddhn_high_u16","Rounding Add returning High Narrow (high half)."],["vraddhn_high_u32","Rounding Add returning High Narrow (high half)."],["vraddhn_high_u64","Rounding Add returning High Narrow (high half)."],["vraddhn_s16","Rounding Add returning High Narrow."],["vraddhn_s32","Rounding Add returning High Narrow."],["vraddhn_s64","Rounding Add returning High Narrow."],["vraddhn_u16","Rounding Add returning High Narrow."],["vraddhn_u32","Rounding Add returning High Narrow."],["vraddhn_u64","Rounding Add returning High Narrow."],["vrecpe_f32","Reciprocal estimate."],["vrecpe_u32","Unsigned reciprocal estimate"],["vrecpeq_f32","Reciprocal estimate."],["vrecpeq_u32","Unsigned reciprocal estimate"],["vrecps_f32","Floating-point reciprocal step"],["vrecpsq_f32","Floating-point reciprocal step"],["vreinterpret_f32_p16","Vector reinterpret cast operation"],["vreinterpret_f32_p8","Vector reinterpret cast operation"],["vreinterpret_f32_s16","Vector reinterpret cast operation"],["vreinterpret_f32_s32","Vector reinterpret cast operation"],["vreinterpret_f32_s64","Vector reinterpret cast operation"],["vreinterpret_f32_s8","Vector reinterpret cast operation"],["vreinterpret_f32_u16","Vector reinterpret cast operation"],["vreinterpret_f32_u32","Vector reinterpret cast operation"],["vreinterpret_f32_u64","Vector reinterpret cast operation"],["vreinterpret_f32_u8","Vector reinterpret cast operation"],["vreinterpret_p16_f32","Vector reinterpret cast operation"],["vreinterpret_p16_p64","Vector reinterpret cast operation"],["vreinterpret_p16_p8","Vector reinterpret cast operation"],["vreinterpret_p16_s16","Vector reinterpret cast operation"],["vreinterpret_p16_s32","Vector reinterpret cast operation"],["vreinterpret_p16_s64","Vector reinterpret cast operation"],["vreinterpret_p16_s8","Vector reinterpret cast operation"],["vreinterpret_p16_u16","Vector reinterpret cast operation"],["vreinterpret_p16_u32","Vector reinterpret cast operation"],["vreinterpret_p16_u64","Vector reinterpret cast operation"],["vreinterpret_p16_u8","Vector reinterpret cast operation"],["vreinterpret_p64_p16","Vector reinterpret cast operation"],["vreinterpret_p64_p8","Vector reinterpret cast operation"],["vreinterpret_p64_s16","Vector reinterpret cast operation"],["vreinterpret_p64_s32","Vector reinterpret cast operation"],["vreinterpret_p64_s8","Vector reinterpret cast operation"],["vreinterpret_p64_u16","Vector reinterpret cast operation"],["vreinterpret_p64_u32","Vector reinterpret cast operation"],["vreinterpret_p64_u8","Vector reinterpret cast operation"],["vreinterpret_p8_f32","Vector reinterpret cast operation"],["vreinterpret_p8_p16","Vector reinterpret cast operation"],["vreinterpret_p8_p64","Vector reinterpret cast operation"],["vreinterpret_p8_s16","Vector reinterpret cast operation"],["vreinterpret_p8_s32","Vector reinterpret cast operation"],["vreinterpret_p8_s64","Vector reinterpret cast operation"],["vreinterpret_p8_s8","Vector reinterpret cast operation"],["vreinterpret_p8_u16","Vector reinterpret cast operation"],["vreinterpret_p8_u32","Vector reinterpret cast operation"],["vreinterpret_p8_u64","Vector reinterpret cast operation"],["vreinterpret_p8_u8","Vector reinterpret cast operation"],["vreinterpret_s16_f32","Vector reinterpret cast operation"],["vreinterpret_s16_p16","Vector reinterpret cast operation"],["vreinterpret_s16_p64","Vector reinterpret cast operation"],["vreinterpret_s16_p8","Vector reinterpret cast operation"],["vreinterpret_s16_s32","Vector reinterpret cast operation"],["vreinterpret_s16_s64","Vector reinterpret cast operation"],["vreinterpret_s16_s8","Vector reinterpret cast operation"],["vreinterpret_s16_u16","Vector reinterpret cast operation"],["vreinterpret_s16_u32","Vector reinterpret cast operation"],["vreinterpret_s16_u64","Vector reinterpret cast operation"],["vreinterpret_s16_u8","Vector reinterpret cast operation"],["vreinterpret_s32_f32","Vector reinterpret cast operation"],["vreinterpret_s32_p16","Vector reinterpret cast operation"],["vreinterpret_s32_p64","Vector reinterpret cast operation"],["vreinterpret_s32_p8","Vector reinterpret cast operation"],["vreinterpret_s32_s16","Vector reinterpret cast operation"],["vreinterpret_s32_s64","Vector reinterpret cast operation"],["vreinterpret_s32_s8","Vector reinterpret cast operation"],["vreinterpret_s32_u16","Vector reinterpret cast operation"],["vreinterpret_s32_u32","Vector reinterpret cast operation"],["vreinterpret_s32_u64","Vector reinterpret cast operation"],["vreinterpret_s32_u8","Vector reinterpret cast operation"],["vreinterpret_s64_f32","Vector reinterpret cast operation"],["vreinterpret_s64_p16","Vector reinterpret cast operation"],["vreinterpret_s64_p8","Vector reinterpret cast operation"],["vreinterpret_s64_s16","Vector reinterpret cast operation"],["vreinterpret_s64_s32","Vector reinterpret cast operation"],["vreinterpret_s64_s8","Vector reinterpret cast operation"],["vreinterpret_s64_u16","Vector reinterpret cast operation"],["vreinterpret_s64_u32","Vector reinterpret cast operation"],["vreinterpret_s64_u64","Vector reinterpret cast operation"],["vreinterpret_s64_u8","Vector reinterpret cast operation"],["vreinterpret_s8_f32","Vector reinterpret cast operation"],["vreinterpret_s8_p16","Vector reinterpret cast operation"],["vreinterpret_s8_p64","Vector reinterpret cast operation"],["vreinterpret_s8_p8","Vector reinterpret cast operation"],["vreinterpret_s8_s16","Vector reinterpret cast operation"],["vreinterpret_s8_s32","Vector reinterpret cast operation"],["vreinterpret_s8_s64","Vector reinterpret cast operation"],["vreinterpret_s8_u16","Vector reinterpret cast operation"],["vreinterpret_s8_u32","Vector reinterpret cast operation"],["vreinterpret_s8_u64","Vector reinterpret cast operation"],["vreinterpret_s8_u8","Vector reinterpret cast operation"],["vreinterpret_u16_f32","Vector reinterpret cast operation"],["vreinterpret_u16_p16","Vector reinterpret cast operation"],["vreinterpret_u16_p64","Vector reinterpret cast operation"],["vreinterpret_u16_p8","Vector reinterpret cast operation"],["vreinterpret_u16_s16","Vector reinterpret cast operation"],["vreinterpret_u16_s32","Vector reinterpret cast operation"],["vreinterpret_u16_s64","Vector reinterpret cast operation"],["vreinterpret_u16_s8","Vector reinterpret cast operation"],["vreinterpret_u16_u32","Vector reinterpret cast operation"],["vreinterpret_u16_u64","Vector reinterpret cast operation"],["vreinterpret_u16_u8","Vector reinterpret cast operation"],["vreinterpret_u32_f32","Vector reinterpret cast operation"],["vreinterpret_u32_p16","Vector reinterpret cast operation"],["vreinterpret_u32_p64","Vector reinterpret cast operation"],["vreinterpret_u32_p8","Vector reinterpret cast operation"],["vreinterpret_u32_s16","Vector reinterpret cast operation"],["vreinterpret_u32_s32","Vector reinterpret cast operation"],["vreinterpret_u32_s64","Vector reinterpret cast operation"],["vreinterpret_u32_s8","Vector reinterpret cast operation"],["vreinterpret_u32_u16","Vector reinterpret cast operation"],["vreinterpret_u32_u64","Vector reinterpret cast operation"],["vreinterpret_u32_u8","Vector reinterpret cast operation"],["vreinterpret_u64_f32","Vector reinterpret cast operation"],["vreinterpret_u64_p16","Vector reinterpret cast operation"],["vreinterpret_u64_p8","Vector reinterpret cast operation"],["vreinterpret_u64_s16","Vector reinterpret cast operation"],["vreinterpret_u64_s32","Vector reinterpret cast operation"],["vreinterpret_u64_s64","Vector reinterpret cast operation"],["vreinterpret_u64_s8","Vector reinterpret cast operation"],["vreinterpret_u64_u16","Vector reinterpret cast operation"],["vreinterpret_u64_u32","Vector reinterpret cast operation"],["vreinterpret_u64_u8","Vector reinterpret cast operation"],["vreinterpret_u8_f32","Vector reinterpret cast operation"],["vreinterpret_u8_p16","Vector reinterpret cast operation"],["vreinterpret_u8_p64","Vector reinterpret cast operation"],["vreinterpret_u8_p8","Vector reinterpret cast operation"],["vreinterpret_u8_s16","Vector reinterpret cast operation"],["vreinterpret_u8_s32","Vector reinterpret cast operation"],["vreinterpret_u8_s64","Vector reinterpret cast operation"],["vreinterpret_u8_s8","Vector reinterpret cast operation"],["vreinterpret_u8_u16","Vector reinterpret cast operation"],["vreinterpret_u8_u32","Vector reinterpret cast operation"],["vreinterpret_u8_u64","Vector reinterpret cast operation"],["vreinterpretq_f32_p128","Vector reinterpret cast operation"],["vreinterpretq_f32_p16","Vector reinterpret cast operation"],["vreinterpretq_f32_p8","Vector reinterpret cast operation"],["vreinterpretq_f32_s16","Vector reinterpret cast operation"],["vreinterpretq_f32_s32","Vector reinterpret cast operation"],["vreinterpretq_f32_s64","Vector reinterpret cast operation"],["vreinterpretq_f32_s8","Vector reinterpret cast operation"],["vreinterpretq_f32_u16","Vector reinterpret cast operation"],["vreinterpretq_f32_u32","Vector reinterpret cast operation"],["vreinterpretq_f32_u64","Vector reinterpret cast operation"],["vreinterpretq_f32_u8","Vector reinterpret cast operation"],["vreinterpretq_p128_f32","Vector reinterpret cast operation"],["vreinterpretq_p128_p16","Vector reinterpret cast operation"],["vreinterpretq_p128_p64","Vector reinterpret cast operation"],["vreinterpretq_p128_p8","Vector reinterpret cast operation"],["vreinterpretq_p128_s16","Vector reinterpret cast operation"],["vreinterpretq_p128_s32","Vector reinterpret cast operation"],["vreinterpretq_p128_s64","Vector reinterpret cast operation"],["vreinterpretq_p128_s8","Vector reinterpret cast operation"],["vreinterpretq_p128_u16","Vector reinterpret cast operation"],["vreinterpretq_p128_u32","Vector reinterpret cast operation"],["vreinterpretq_p128_u64","Vector reinterpret cast operation"],["vreinterpretq_p128_u8","Vector reinterpret cast operation"],["vreinterpretq_p16_f32","Vector reinterpret cast operation"],["vreinterpretq_p16_p128","Vector reinterpret cast operation"],["vreinterpretq_p16_p64","Vector reinterpret cast operation"],["vreinterpretq_p16_p8","Vector reinterpret cast operation"],["vreinterpretq_p16_s16","Vector reinterpret cast operation"],["vreinterpretq_p16_s32","Vector reinterpret cast operation"],["vreinterpretq_p16_s64","Vector reinterpret cast operation"],["vreinterpretq_p16_s8","Vector reinterpret cast operation"],["vreinterpretq_p16_u16","Vector reinterpret cast operation"],["vreinterpretq_p16_u32","Vector reinterpret cast operation"],["vreinterpretq_p16_u64","Vector reinterpret cast operation"],["vreinterpretq_p16_u8","Vector reinterpret cast operation"],["vreinterpretq_p64_p128","Vector reinterpret cast operation"],["vreinterpretq_p64_p16","Vector reinterpret cast operation"],["vreinterpretq_p64_p8","Vector reinterpret cast operation"],["vreinterpretq_p64_s16","Vector reinterpret cast operation"],["vreinterpretq_p64_s32","Vector reinterpret cast operation"],["vreinterpretq_p64_s8","Vector reinterpret cast operation"],["vreinterpretq_p64_u16","Vector reinterpret cast operation"],["vreinterpretq_p64_u32","Vector reinterpret cast operation"],["vreinterpretq_p64_u8","Vector reinterpret cast operation"],["vreinterpretq_p8_f32","Vector reinterpret cast operation"],["vreinterpretq_p8_p128","Vector reinterpret cast operation"],["vreinterpretq_p8_p16","Vector reinterpret cast operation"],["vreinterpretq_p8_p64","Vector reinterpret cast operation"],["vreinterpretq_p8_s16","Vector reinterpret cast operation"],["vreinterpretq_p8_s32","Vector reinterpret cast operation"],["vreinterpretq_p8_s64","Vector reinterpret cast operation"],["vreinterpretq_p8_s8","Vector reinterpret cast operation"],["vreinterpretq_p8_u16","Vector reinterpret cast operation"],["vreinterpretq_p8_u32","Vector reinterpret cast operation"],["vreinterpretq_p8_u64","Vector reinterpret cast operation"],["vreinterpretq_p8_u8","Vector reinterpret cast operation"],["vreinterpretq_s16_f32","Vector reinterpret cast operation"],["vreinterpretq_s16_p128","Vector reinterpret cast operation"],["vreinterpretq_s16_p16","Vector reinterpret cast operation"],["vreinterpretq_s16_p64","Vector reinterpret cast operation"],["vreinterpretq_s16_p8","Vector reinterpret cast operation"],["vreinterpretq_s16_s32","Vector reinterpret cast operation"],["vreinterpretq_s16_s64","Vector reinterpret cast operation"],["vreinterpretq_s16_s8","Vector reinterpret cast operation"],["vreinterpretq_s16_u16","Vector reinterpret cast operation"],["vreinterpretq_s16_u32","Vector reinterpret cast operation"],["vreinterpretq_s16_u64","Vector reinterpret cast operation"],["vreinterpretq_s16_u8","Vector reinterpret cast operation"],["vreinterpretq_s32_f32","Vector reinterpret cast operation"],["vreinterpretq_s32_p128","Vector reinterpret cast operation"],["vreinterpretq_s32_p16","Vector reinterpret cast operation"],["vreinterpretq_s32_p64","Vector reinterpret cast operation"],["vreinterpretq_s32_p8","Vector reinterpret cast operation"],["vreinterpretq_s32_s16","Vector reinterpret cast operation"],["vreinterpretq_s32_s64","Vector reinterpret cast operation"],["vreinterpretq_s32_s8","Vector reinterpret cast operation"],["vreinterpretq_s32_u16","Vector reinterpret cast operation"],["vreinterpretq_s32_u32","Vector reinterpret cast operation"],["vreinterpretq_s32_u64","Vector reinterpret cast operation"],["vreinterpretq_s32_u8","Vector reinterpret cast operation"],["vreinterpretq_s64_f32","Vector reinterpret cast operation"],["vreinterpretq_s64_p128","Vector reinterpret cast operation"],["vreinterpretq_s64_p16","Vector reinterpret cast operation"],["vreinterpretq_s64_p8","Vector reinterpret cast operation"],["vreinterpretq_s64_s16","Vector reinterpret cast operation"],["vreinterpretq_s64_s32","Vector reinterpret cast operation"],["vreinterpretq_s64_s8","Vector reinterpret cast operation"],["vreinterpretq_s64_u16","Vector reinterpret cast operation"],["vreinterpretq_s64_u32","Vector reinterpret cast operation"],["vreinterpretq_s64_u64","Vector reinterpret cast operation"],["vreinterpretq_s64_u8","Vector reinterpret cast operation"],["vreinterpretq_s8_f32","Vector reinterpret cast operation"],["vreinterpretq_s8_p128","Vector reinterpret cast operation"],["vreinterpretq_s8_p16","Vector reinterpret cast operation"],["vreinterpretq_s8_p64","Vector reinterpret cast operation"],["vreinterpretq_s8_p8","Vector reinterpret cast operation"],["vreinterpretq_s8_s16","Vector reinterpret cast operation"],["vreinterpretq_s8_s32","Vector reinterpret cast operation"],["vreinterpretq_s8_s64","Vector reinterpret cast operation"],["vreinterpretq_s8_u16","Vector reinterpret cast operation"],["vreinterpretq_s8_u32","Vector reinterpret cast operation"],["vreinterpretq_s8_u64","Vector reinterpret cast operation"],["vreinterpretq_s8_u8","Vector reinterpret cast operation"],["vreinterpretq_u16_f32","Vector reinterpret cast operation"],["vreinterpretq_u16_p128","Vector reinterpret cast operation"],["vreinterpretq_u16_p16","Vector reinterpret cast operation"],["vreinterpretq_u16_p64","Vector reinterpret cast operation"],["vreinterpretq_u16_p8","Vector reinterpret cast operation"],["vreinterpretq_u16_s16","Vector reinterpret cast operation"],["vreinterpretq_u16_s32","Vector reinterpret cast operation"],["vreinterpretq_u16_s64","Vector reinterpret cast operation"],["vreinterpretq_u16_s8","Vector reinterpret cast operation"],["vreinterpretq_u16_u32","Vector reinterpret cast operation"],["vreinterpretq_u16_u64","Vector reinterpret cast operation"],["vreinterpretq_u16_u8","Vector reinterpret cast operation"],["vreinterpretq_u32_f32","Vector reinterpret cast operation"],["vreinterpretq_u32_p128","Vector reinterpret cast operation"],["vreinterpretq_u32_p16","Vector reinterpret cast operation"],["vreinterpretq_u32_p64","Vector reinterpret cast operation"],["vreinterpretq_u32_p8","Vector reinterpret cast operation"],["vreinterpretq_u32_s16","Vector reinterpret cast operation"],["vreinterpretq_u32_s32","Vector reinterpret cast operation"],["vreinterpretq_u32_s64","Vector reinterpret cast operation"],["vreinterpretq_u32_s8","Vector reinterpret cast operation"],["vreinterpretq_u32_u16","Vector reinterpret cast operation"],["vreinterpretq_u32_u64","Vector reinterpret cast operation"],["vreinterpretq_u32_u8","Vector reinterpret cast operation"],["vreinterpretq_u64_f32","Vector reinterpret cast operation"],["vreinterpretq_u64_p128","Vector reinterpret cast operation"],["vreinterpretq_u64_p16","Vector reinterpret cast operation"],["vreinterpretq_u64_p8","Vector reinterpret cast operation"],["vreinterpretq_u64_s16","Vector reinterpret cast operation"],["vreinterpretq_u64_s32","Vector reinterpret cast operation"],["vreinterpretq_u64_s64","Vector reinterpret cast operation"],["vreinterpretq_u64_s8","Vector reinterpret cast operation"],["vreinterpretq_u64_u16","Vector reinterpret cast operation"],["vreinterpretq_u64_u32","Vector reinterpret cast operation"],["vreinterpretq_u64_u8","Vector reinterpret cast operation"],["vreinterpretq_u8_f32","Vector reinterpret cast operation"],["vreinterpretq_u8_p128","Vector reinterpret cast operation"],["vreinterpretq_u8_p16","Vector reinterpret cast operation"],["vreinterpretq_u8_p64","Vector reinterpret cast operation"],["vreinterpretq_u8_p8","Vector reinterpret cast operation"],["vreinterpretq_u8_s16","Vector reinterpret cast operation"],["vreinterpretq_u8_s32","Vector reinterpret cast operation"],["vreinterpretq_u8_s64","Vector reinterpret cast operation"],["vreinterpretq_u8_s8","Vector reinterpret cast operation"],["vreinterpretq_u8_u16","Vector reinterpret cast operation"],["vreinterpretq_u8_u32","Vector reinterpret cast operation"],["vreinterpretq_u8_u64","Vector reinterpret cast operation"],["vrev16_p8","Reversing vector elements (swap endianness)"],["vrev16_s8","Reversing vector elements (swap endianness)"],["vrev16_u8","Reversing vector elements (swap endianness)"],["vrev16q_p8","Reversing vector elements (swap endianness)"],["vrev16q_s8","Reversing vector elements (swap endianness)"],["vrev16q_u8","Reversing vector elements (swap endianness)"],["vrev32_p16","Reversing vector elements (swap endianness)"],["vrev32_p8","Reversing vector elements (swap endianness)"],["vrev32_s16","Reversing vector elements (swap endianness)"],["vrev32_s8","Reversing vector elements (swap endianness)"],["vrev32_u16","Reversing vector elements (swap endianness)"],["vrev32_u8","Reversing vector elements (swap endianness)"],["vrev32q_p16","Reversing vector elements (swap endianness)"],["vrev32q_p8","Reversing vector elements (swap endianness)"],["vrev32q_s16","Reversing vector elements (swap endianness)"],["vrev32q_s8","Reversing vector elements (swap endianness)"],["vrev32q_u16","Reversing vector elements (swap endianness)"],["vrev32q_u8","Reversing vector elements (swap endianness)"],["vrev64_f32","Reversing vector elements (swap endianness)"],["vrev64_p16","Reversing vector elements (swap endianness)"],["vrev64_p8","Reversing vector elements (swap endianness)"],["vrev64_s16","Reversing vector elements (swap endianness)"],["vrev64_s32","Reversing vector elements (swap endianness)"],["vrev64_s8","Reversing vector elements (swap endianness)"],["vrev64_u16","Reversing vector elements (swap endianness)"],["vrev64_u32","Reversing vector elements (swap endianness)"],["vrev64_u8","Reversing vector elements (swap endianness)"],["vrev64q_f32","Reversing vector elements (swap endianness)"],["vrev64q_p16","Reversing vector elements (swap endianness)"],["vrev64q_p8","Reversing vector elements (swap endianness)"],["vrev64q_s16","Reversing vector elements (swap endianness)"],["vrev64q_s32","Reversing vector elements (swap endianness)"],["vrev64q_s8","Reversing vector elements (swap endianness)"],["vrev64q_u16","Reversing vector elements (swap endianness)"],["vrev64q_u32","Reversing vector elements (swap endianness)"],["vrev64q_u8","Reversing vector elements (swap endianness)"],["vrhadd_s16","Rounding halving add"],["vrhadd_s32","Rounding halving add"],["vrhadd_s8","Rounding halving add"],["vrhadd_u16","Rounding halving add"],["vrhadd_u32","Rounding halving add"],["vrhadd_u8","Rounding halving add"],["vrhaddq_s16","Rounding halving add"],["vrhaddq_s32","Rounding halving add"],["vrhaddq_s8","Rounding halving add"],["vrhaddq_u16","Rounding halving add"],["vrhaddq_u32","Rounding halving add"],["vrhaddq_u8","Rounding halving add"],["vrndn_f32","Floating-point round to integral, to nearest with ties to even"],["vrndnq_f32","Floating-point round to integral, to nearest with ties to even"],["vrshl_s16","Signed rounding shift left"],["vrshl_s32","Signed rounding shift left"],["vrshl_s64","Signed rounding shift left"],["vrshl_s8","Signed rounding shift left"],["vrshl_u16","Unsigned rounding shift left"],["vrshl_u32","Unsigned rounding shift left"],["vrshl_u64","Unsigned rounding shift left"],["vrshl_u8","Unsigned rounding shift left"],["vrshlq_s16","Signed rounding shift left"],["vrshlq_s32","Signed rounding shift left"],["vrshlq_s64","Signed rounding shift left"],["vrshlq_s8","Signed rounding shift left"],["vrshlq_u16","Unsigned rounding shift left"],["vrshlq_u32","Unsigned rounding shift left"],["vrshlq_u64","Unsigned rounding shift left"],["vrshlq_u8","Unsigned rounding shift left"],["vrshr_n_s16","Signed rounding shift right"],["vrshr_n_s32","Signed rounding shift right"],["vrshr_n_s64","Signed rounding shift right"],["vrshr_n_s8","Signed rounding shift right"],["vrshr_n_u16","Unsigned rounding shift right"],["vrshr_n_u32","Unsigned rounding shift right"],["vrshr_n_u64","Unsigned rounding shift right"],["vrshr_n_u8","Unsigned rounding shift right"],["vrshrn_n_u16","Rounding shift right narrow"],["vrshrn_n_u32","Rounding shift right narrow"],["vrshrn_n_u64","Rounding shift right narrow"],["vrshrq_n_s16","Signed rounding shift right"],["vrshrq_n_s32","Signed rounding shift right"],["vrshrq_n_s64","Signed rounding shift right"],["vrshrq_n_s8","Signed rounding shift right"],["vrshrq_n_u16","Unsigned rounding shift right"],["vrshrq_n_u32","Unsigned rounding shift right"],["vrshrq_n_u64","Unsigned rounding shift right"],["vrshrq_n_u8","Unsigned rounding shift right"],["vrsqrte_f32","Reciprocal square-root estimate."],["vrsqrte_u32","Unsigned reciprocal square root estimate"],["vrsqrteq_f32","Reciprocal square-root estimate."],["vrsqrteq_u32","Unsigned reciprocal square root estimate"],["vrsqrts_f32","Floating-point reciprocal square root step"],["vrsqrtsq_f32","Floating-point reciprocal square root step"],["vrsra_n_s16","Signed rounding shift right and accumulate"],["vrsra_n_s32","Signed rounding shift right and accumulate"],["vrsra_n_s64","Signed rounding shift right and accumulate"],["vrsra_n_s8","Signed rounding shift right and accumulate"],["vrsra_n_u16","Unsigned rounding shift right and accumulate"],["vrsra_n_u32","Unsigned rounding shift right and accumulate"],["vrsra_n_u64","Unsigned rounding shift right and accumulate"],["vrsra_n_u8","Unsigned rounding shift right and accumulate"],["vrsraq_n_s16","Signed rounding shift right and accumulate"],["vrsraq_n_s32","Signed rounding shift right and accumulate"],["vrsraq_n_s64","Signed rounding shift right and accumulate"],["vrsraq_n_s8","Signed rounding shift right and accumulate"],["vrsraq_n_u16","Unsigned rounding shift right and accumulate"],["vrsraq_n_u32","Unsigned rounding shift right and accumulate"],["vrsraq_n_u64","Unsigned rounding shift right and accumulate"],["vrsraq_n_u8","Unsigned rounding shift right and accumulate"],["vrsubhn_s16","Rounding subtract returning high narrow"],["vrsubhn_s32","Rounding subtract returning high narrow"],["vrsubhn_s64","Rounding subtract returning high narrow"],["vrsubhn_u16","Rounding subtract returning high narrow"],["vrsubhn_u32","Rounding subtract returning high narrow"],["vrsubhn_u64","Rounding subtract returning high narrow"],["vset_lane_f32","Insert vector element from another vector element"],["vset_lane_p16","Insert vector element from another vector element"],["vset_lane_p64","Insert vector element from another vector element"],["vset_lane_p8","Insert vector element from another vector element"],["vset_lane_s16","Insert vector element from another vector element"],["vset_lane_s32","Insert vector element from another vector element"],["vset_lane_s64","Insert vector element from another vector element"],["vset_lane_s8","Insert vector element from another vector element"],["vset_lane_u16","Insert vector element from another vector element"],["vset_lane_u32","Insert vector element from another vector element"],["vset_lane_u64","Insert vector element from another vector element"],["vset_lane_u8","Insert vector element from another vector element"],["vsetq_lane_f32","Insert vector element from another vector element"],["vsetq_lane_p16","Insert vector element from another vector element"],["vsetq_lane_p64","Insert vector element from another vector element"],["vsetq_lane_p8","Insert vector element from another vector element"],["vsetq_lane_s16","Insert vector element from another vector element"],["vsetq_lane_s32","Insert vector element from another vector element"],["vsetq_lane_s64","Insert vector element from another vector element"],["vsetq_lane_s8","Insert vector element from another vector element"],["vsetq_lane_u16","Insert vector element from another vector element"],["vsetq_lane_u32","Insert vector element from another vector element"],["vsetq_lane_u64","Insert vector element from another vector element"],["vsetq_lane_u8","Insert vector element from another vector element"],["vsha1cq_u32","SHA1 hash update accelerator, choose."],["vsha1h_u32","SHA1 fixed rotate."],["vsha1mq_u32","SHA1 hash update accelerator, majority."],["vsha1pq_u32","SHA1 hash update accelerator, parity."],["vsha1su0q_u32","SHA1 schedule update accelerator, first part."],["vsha1su1q_u32","SHA1 schedule update accelerator, second part."],["vsha256h2q_u32","SHA256 hash update accelerator, upper part."],["vsha256hq_u32","SHA256 hash update accelerator."],["vsha256su0q_u32","SHA256 schedule update accelerator, first part."],["vsha256su1q_u32","SHA256 schedule update accelerator, second part."],["vshl_n_s16","Shift left"],["vshl_n_s32","Shift left"],["vshl_n_s64","Shift left"],["vshl_n_s8","Shift left"],["vshl_n_u16","Shift left"],["vshl_n_u32","Shift left"],["vshl_n_u64","Shift left"],["vshl_n_u8","Shift left"],["vshl_s16","Signed Shift left"],["vshl_s32","Signed Shift left"],["vshl_s64","Signed Shift left"],["vshl_s8","Signed Shift left"],["vshl_u16","Unsigned Shift left"],["vshl_u32","Unsigned Shift left"],["vshl_u64","Unsigned Shift left"],["vshl_u8","Unsigned Shift left"],["vshll_n_s16","Signed shift left long"],["vshll_n_s32","Signed shift left long"],["vshll_n_s8","Signed shift left long"],["vshll_n_u16","Signed shift left long"],["vshll_n_u32","Signed shift left long"],["vshll_n_u8","Signed shift left long"],["vshlq_n_s16","Shift left"],["vshlq_n_s32","Shift left"],["vshlq_n_s64","Shift left"],["vshlq_n_s8","Shift left"],["vshlq_n_u16","Shift left"],["vshlq_n_u32","Shift left"],["vshlq_n_u64","Shift left"],["vshlq_n_u8","Shift left"],["vshlq_s16","Signed Shift left"],["vshlq_s32","Signed Shift left"],["vshlq_s64","Signed Shift left"],["vshlq_s8","Signed Shift left"],["vshlq_u16","Unsigned Shift left"],["vshlq_u32","Unsigned Shift left"],["vshlq_u64","Unsigned Shift left"],["vshlq_u8","Unsigned Shift left"],["vshr_n_s16","Shift right"],["vshr_n_s32","Shift right"],["vshr_n_s64","Shift right"],["vshr_n_s8","Shift right"],["vshr_n_u16","Shift right"],["vshr_n_u32","Shift right"],["vshr_n_u64","Shift right"],["vshr_n_u8","Shift right"],["vshrn_n_s16","Shift right narrow"],["vshrn_n_s32","Shift right narrow"],["vshrn_n_s64","Shift right narrow"],["vshrn_n_u16","Shift right narrow"],["vshrn_n_u32","Shift right narrow"],["vshrn_n_u64","Shift right narrow"],["vshrq_n_s16","Shift right"],["vshrq_n_s32","Shift right"],["vshrq_n_s64","Shift right"],["vshrq_n_s8","Shift right"],["vshrq_n_u16","Shift right"],["vshrq_n_u32","Shift right"],["vshrq_n_u64","Shift right"],["vshrq_n_u8","Shift right"],["vsli_n_p16","Shift Left and Insert (immediate)"],["vsli_n_p64","Shift Left and Insert (immediate)"],["vsli_n_p8","Shift Left and Insert (immediate)"],["vsli_n_s16","Shift Left and Insert (immediate)"],["vsli_n_s32","Shift Left and Insert (immediate)"],["vsli_n_s64","Shift Left and Insert (immediate)"],["vsli_n_s8","Shift Left and Insert (immediate)"],["vsli_n_u16","Shift Left and Insert (immediate)"],["vsli_n_u32","Shift Left and Insert (immediate)"],["vsli_n_u64","Shift Left and Insert (immediate)"],["vsli_n_u8","Shift Left and Insert (immediate)"],["vsliq_n_p16","Shift Left and Insert (immediate)"],["vsliq_n_p64","Shift Left and Insert (immediate)"],["vsliq_n_p8","Shift Left and Insert (immediate)"],["vsliq_n_s16","Shift Left and Insert (immediate)"],["vsliq_n_s32","Shift Left and Insert (immediate)"],["vsliq_n_s64","Shift Left and Insert (immediate)"],["vsliq_n_s8","Shift Left and Insert (immediate)"],["vsliq_n_u16","Shift Left and Insert (immediate)"],["vsliq_n_u32","Shift Left and Insert (immediate)"],["vsliq_n_u64","Shift Left and Insert (immediate)"],["vsliq_n_u8","Shift Left and Insert (immediate)"],["vsra_n_s16","Signed shift right and accumulate"],["vsra_n_s32","Signed shift right and accumulate"],["vsra_n_s64","Signed shift right and accumulate"],["vsra_n_s8","Signed shift right and accumulate"],["vsra_n_u16","Unsigned shift right and accumulate"],["vsra_n_u32","Unsigned shift right and accumulate"],["vsra_n_u64","Unsigned shift right and accumulate"],["vsra_n_u8","Unsigned shift right and accumulate"],["vsraq_n_s16","Signed shift right and accumulate"],["vsraq_n_s32","Signed shift right and accumulate"],["vsraq_n_s64","Signed shift right and accumulate"],["vsraq_n_s8","Signed shift right and accumulate"],["vsraq_n_u16","Unsigned shift right and accumulate"],["vsraq_n_u32","Unsigned shift right and accumulate"],["vsraq_n_u64","Unsigned shift right and accumulate"],["vsraq_n_u8","Unsigned shift right and accumulate"],["vsri_n_p16","Shift Right and Insert (immediate)"],["vsri_n_p64","Shift Right and Insert (immediate)"],["vsri_n_p8","Shift Right and Insert (immediate)"],["vsri_n_s16","Shift Right and Insert (immediate)"],["vsri_n_s32","Shift Right and Insert (immediate)"],["vsri_n_s64","Shift Right and Insert (immediate)"],["vsri_n_s8","Shift Right and Insert (immediate)"],["vsri_n_u16","Shift Right and Insert (immediate)"],["vsri_n_u32","Shift Right and Insert (immediate)"],["vsri_n_u64","Shift Right and Insert (immediate)"],["vsri_n_u8","Shift Right and Insert (immediate)"],["vsriq_n_p16","Shift Right and Insert (immediate)"],["vsriq_n_p64","Shift Right and Insert (immediate)"],["vsriq_n_p8","Shift Right and Insert (immediate)"],["vsriq_n_s16","Shift Right and Insert (immediate)"],["vsriq_n_s32","Shift Right and Insert (immediate)"],["vsriq_n_s64","Shift Right and Insert (immediate)"],["vsriq_n_s8","Shift Right and Insert (immediate)"],["vsriq_n_u16","Shift Right and Insert (immediate)"],["vsriq_n_u32","Shift Right and Insert (immediate)"],["vsriq_n_u64","Shift Right and Insert (immediate)"],["vsriq_n_u8","Shift Right and Insert (immediate)"],["vst1_f32",""],["vst1_lane_f32","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_p16","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_p64","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_p8","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_s16","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_s32","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_s64","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_s8","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_u16","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_u32","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_u64","Store multiple single-element structures from one, two, three, or four registers"],["vst1_lane_u8","Store multiple single-element structures from one, two, three, or four registers"],["vst1_p16","Store multiple single-element structures from one, two, three, or four registers."],["vst1_p16_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1_p16_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1_p16_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1_p64","Store multiple single-element structures from one, two, three, or four registers."],["vst1_p64_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1_p64_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1_p64_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1_p8","Store multiple single-element structures from one, two, three, or four registers."],["vst1_p8_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1_p8_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1_p8_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1_s16","Store multiple single-element structures from one, two, three, or four registers."],["vst1_s32","Store multiple single-element structures from one, two, three, or four registers."],["vst1_s64","Store multiple single-element structures from one, two, three, or four registers."],["vst1_s8","Store multiple single-element structures from one, two, three, or four registers."],["vst1_u16","Store multiple single-element structures from one, two, three, or four registers."],["vst1_u16_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u16_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u16_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u32","Store multiple single-element structures from one, two, three, or four registers."],["vst1_u32_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u32_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u32_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u64","Store multiple single-element structures from one, two, three, or four registers."],["vst1_u64_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u64_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u64_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u8","Store multiple single-element structures from one, two, three, or four registers."],["vst1_u8_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u8_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1_u8_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_f32",""],["vst1q_lane_f32","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_p16","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_p64","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_p8","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_s16","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_s32","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_s64","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_s8","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_u16","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_u32","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_u64","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_lane_u8","Store multiple single-element structures from one, two, three, or four registers"],["vst1q_p16","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_p16_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_p16_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_p16_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_p64","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_p64_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_p64_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_p64_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_p8","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_p8_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_p8_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_p8_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_s16","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_s32","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_s64","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_s8","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_u16","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_u16_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u16_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u16_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u32","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_u32_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u32_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u32_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u64","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_u64_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u64_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u64_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u8","Store multiple single-element structures from one, two, three, or four registers."],["vst1q_u8_x2","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u8_x3","Store multiple single-element structures to one, two, three, or four registers"],["vst1q_u8_x4","Store multiple single-element structures to one, two, three, or four registers"],["vst2_lane_p16","Store multiple 2-element structures from two registers"],["vst2_lane_p8","Store multiple 2-element structures from two registers"],["vst2_lane_u16","Store multiple 2-element structures from two registers"],["vst2_lane_u32","Store multiple 2-element structures from two registers"],["vst2_lane_u8","Store multiple 2-element structures from two registers"],["vst2_p16","Store multiple 2-element structures from two registers"],["vst2_p64","Store multiple 2-element structures from two registers"],["vst2_p8","Store multiple 2-element structures from two registers"],["vst2_u16","Store multiple 2-element structures from two registers"],["vst2_u32","Store multiple 2-element structures from two registers"],["vst2_u64","Store multiple 2-element structures from two registers"],["vst2_u8","Store multiple 2-element structures from two registers"],["vst2q_lane_p16","Store multiple 2-element structures from two registers"],["vst2q_lane_u16","Store multiple 2-element structures from two registers"],["vst2q_lane_u32","Store multiple 2-element structures from two registers"],["vst2q_p16","Store multiple 2-element structures from two registers"],["vst2q_p8","Store multiple 2-element structures from two registers"],["vst2q_u16","Store multiple 2-element structures from two registers"],["vst2q_u32","Store multiple 2-element structures from two registers"],["vst2q_u8","Store multiple 2-element structures from two registers"],["vst3_lane_p16","Store multiple 3-element structures from three registers"],["vst3_lane_p8","Store multiple 3-element structures from three registers"],["vst3_lane_u16","Store multiple 3-element structures from three registers"],["vst3_lane_u32","Store multiple 3-element structures from three registers"],["vst3_lane_u8","Store multiple 3-element structures from three registers"],["vst3_p16","Store multiple 3-element structures from three registers"],["vst3_p64","Store multiple 3-element structures from three registers"],["vst3_p8","Store multiple 3-element structures from three registers"],["vst3_u16","Store multiple 3-element structures from three registers"],["vst3_u32","Store multiple 3-element structures from three registers"],["vst3_u64","Store multiple 3-element structures from three registers"],["vst3_u8","Store multiple 3-element structures from three registers"],["vst3q_lane_p16","Store multiple 3-element structures from three registers"],["vst3q_lane_u16","Store multiple 3-element structures from three registers"],["vst3q_lane_u32","Store multiple 3-element structures from three registers"],["vst3q_p16","Store multiple 3-element structures from three registers"],["vst3q_p8","Store multiple 3-element structures from three registers"],["vst3q_u16","Store multiple 3-element structures from three registers"],["vst3q_u32","Store multiple 3-element structures from three registers"],["vst3q_u8","Store multiple 3-element structures from three registers"],["vst4_lane_p16","Store multiple 4-element structures from four registers"],["vst4_lane_p8","Store multiple 4-element structures from four registers"],["vst4_lane_u16","Store multiple 4-element structures from four registers"],["vst4_lane_u32","Store multiple 4-element structures from four registers"],["vst4_lane_u8","Store multiple 4-element structures from four registers"],["vst4_p16","Store multiple 4-element structures from four registers"],["vst4_p64","Store multiple 4-element structures from four registers"],["vst4_p8","Store multiple 4-element structures from four registers"],["vst4_u16","Store multiple 4-element structures from four registers"],["vst4_u32","Store multiple 4-element structures from four registers"],["vst4_u64","Store multiple 4-element structures from four registers"],["vst4_u8","Store multiple 4-element structures from four registers"],["vst4q_lane_p16","Store multiple 4-element structures from four registers"],["vst4q_lane_u16","Store multiple 4-element structures from four registers"],["vst4q_lane_u32","Store multiple 4-element structures from four registers"],["vst4q_p16","Store multiple 4-element structures from four registers"],["vst4q_p8","Store multiple 4-element structures from four registers"],["vst4q_u16","Store multiple 4-element structures from four registers"],["vst4q_u32","Store multiple 4-element structures from four registers"],["vst4q_u8","Store multiple 4-element structures from four registers"],["vstrq_p128","Store SIMD&FP register (immediate offset)"],["vsub_f32","Subtract"],["vsub_s16","Subtract"],["vsub_s32","Subtract"],["vsub_s64","Subtract"],["vsub_s8","Subtract"],["vsub_u16","Subtract"],["vsub_u32","Subtract"],["vsub_u64","Subtract"],["vsub_u8","Subtract"],["vsubhn_high_s16","Subtract returning high narrow"],["vsubhn_high_s32","Subtract returning high narrow"],["vsubhn_high_s64","Subtract returning high narrow"],["vsubhn_high_u16","Subtract returning high narrow"],["vsubhn_high_u32","Subtract returning high narrow"],["vsubhn_high_u64","Subtract returning high narrow"],["vsubhn_s16","Subtract returning high narrow"],["vsubhn_s32","Subtract returning high narrow"],["vsubhn_s64","Subtract returning high narrow"],["vsubhn_u16","Subtract returning high narrow"],["vsubhn_u32","Subtract returning high narrow"],["vsubhn_u64","Subtract returning high narrow"],["vsubl_s16","Signed Subtract Long"],["vsubl_s32","Signed Subtract Long"],["vsubl_s8","Signed Subtract Long"],["vsubl_u16","Unsigned Subtract Long"],["vsubl_u32","Unsigned Subtract Long"],["vsubl_u8","Unsigned Subtract Long"],["vsubq_f32","Subtract"],["vsubq_s16","Subtract"],["vsubq_s32","Subtract"],["vsubq_s64","Subtract"],["vsubq_s8","Subtract"],["vsubq_u16","Subtract"],["vsubq_u32","Subtract"],["vsubq_u64","Subtract"],["vsubq_u8","Subtract"],["vsubw_s16","Signed Subtract Wide"],["vsubw_s32","Signed Subtract Wide"],["vsubw_s8","Signed Subtract Wide"],["vsubw_u16","Unsigned Subtract Wide"],["vsubw_u32","Unsigned Subtract Wide"],["vsubw_u8","Unsigned Subtract Wide"],["vtbl1_p8","Table look-up"],["vtbl1_s8","Table look-up"],["vtbl1_u8","Table look-up"],["vtbl2_p8","Table look-up"],["vtbl2_s8","Table look-up"],["vtbl2_u8","Table look-up"],["vtbl3_p8","Table look-up"],["vtbl3_s8","Table look-up"],["vtbl3_u8","Table look-up"],["vtbl4_p8","Table look-up"],["vtbl4_s8","Table look-up"],["vtbl4_u8","Table look-up"],["vtbx1_p8","Extended table look-up"],["vtbx1_s8","Extended table look-up"],["vtbx1_u8","Extended table look-up"],["vtbx2_p8","Extended table look-up"],["vtbx2_s8","Extended table look-up"],["vtbx2_u8","Extended table look-up"],["vtbx3_p8","Extended table look-up"],["vtbx3_s8","Extended table look-up"],["vtbx3_u8","Extended table look-up"],["vtbx4_p8","Extended table look-up"],["vtbx4_s8","Extended table look-up"],["vtbx4_u8","Extended table look-up"],["vtrn_f32","Transpose elements"],["vtrn_p16","Transpose elements"],["vtrn_p8","Transpose elements"],["vtrn_s16","Transpose elements"],["vtrn_s32","Transpose elements"],["vtrn_s8","Transpose elements"],["vtrn_u16","Transpose elements"],["vtrn_u32","Transpose elements"],["vtrn_u8","Transpose elements"],["vtrnq_f32","Transpose elements"],["vtrnq_p16","Transpose elements"],["vtrnq_p8","Transpose elements"],["vtrnq_s16","Transpose elements"],["vtrnq_s32","Transpose elements"],["vtrnq_s8","Transpose elements"],["vtrnq_u16","Transpose elements"],["vtrnq_u32","Transpose elements"],["vtrnq_u8","Transpose elements"],["vtst_p16","Signed compare bitwise Test bits nonzero"],["vtst_p8","Signed compare bitwise Test bits nonzero"],["vtst_s16","Signed compare bitwise Test bits nonzero"],["vtst_s32","Signed compare bitwise Test bits nonzero"],["vtst_s8","Signed compare bitwise Test bits nonzero"],["vtst_u16","Unsigned compare bitwise Test bits nonzero"],["vtst_u32","Unsigned compare bitwise Test bits nonzero"],["vtst_u8","Unsigned compare bitwise Test bits nonzero"],["vtstq_p16","Signed compare bitwise Test bits nonzero"],["vtstq_p8","Signed compare bitwise Test bits nonzero"],["vtstq_s16","Signed compare bitwise Test bits nonzero"],["vtstq_s32","Signed compare bitwise Test bits nonzero"],["vtstq_s8","Signed compare bitwise Test bits nonzero"],["vtstq_u16","Unsigned compare bitwise Test bits nonzero"],["vtstq_u32","Unsigned compare bitwise Test bits nonzero"],["vtstq_u8","Unsigned compare bitwise Test bits nonzero"],["vusmmlaq_s32","Unsigned and signed 8-bit integer matrix multiply-accumulate"],["vuzp_f32","Unzip vectors"],["vuzp_p16","Unzip vectors"],["vuzp_p8","Unzip vectors"],["vuzp_s16","Unzip vectors"],["vuzp_s32","Unzip vectors"],["vuzp_s8","Unzip vectors"],["vuzp_u16","Unzip vectors"],["vuzp_u32","Unzip vectors"],["vuzp_u8","Unzip vectors"],["vuzpq_f32","Unzip vectors"],["vuzpq_p16","Unzip vectors"],["vuzpq_p8","Unzip vectors"],["vuzpq_s16","Unzip vectors"],["vuzpq_s32","Unzip vectors"],["vuzpq_s8","Unzip vectors"],["vuzpq_u16","Unzip vectors"],["vuzpq_u32","Unzip vectors"],["vuzpq_u8","Unzip vectors"],["vzip_f32","Zip vectors"],["vzip_p16","Zip vectors"],["vzip_p8","Zip vectors"],["vzip_s16","Zip vectors"],["vzip_s32","Zip vectors"],["vzip_s8","Zip vectors"],["vzip_u16","Zip vectors"],["vzip_u32","Zip vectors"],["vzip_u8","Zip vectors"],["vzipq_f32","Zip vectors"],["vzipq_p16","Zip vectors"],["vzipq_p8","Zip vectors"],["vzipq_s16","Zip vectors"],["vzipq_s32","Zip vectors"],["vzipq_s8","Zip vectors"],["vzipq_u16","Zip vectors"],["vzipq_u32","Zip vectors"],["vzipq_u8","Zip vectors"]],"mod":[["dsp","References:"]],"struct":[["APSR","Application Program Status Register"],["SY","Full system is the required shareability domain, reads and writes are the required access types"],["float32x2_t","ARM-specific 64-bit wide vector of two packed `f32`."],["float32x2x2_t","ARM-specific type containing two `float32x2_t` vectors."],["float32x2x3_t","ARM-specific type containing three `float32x2_t` vectors."],["float32x2x4_t","ARM-specific type containing four `float32x2_t` vectors."],["float32x4_t","ARM-specific 128-bit wide vector of four packed `f32`."],["float32x4x2_t","ARM-specific type containing two `float32x4_t` vectors."],["float32x4x3_t","ARM-specific type containing three `float32x4_t` vectors."],["float32x4x4_t","ARM-specific type containing four `float32x4_t` vectors."],["int16x2_t","ARM-specific 32-bit wide vector of two packed `i16`."],["int16x4_t","ARM-specific 64-bit wide vector of four packed `i16`."],["int16x4x2_t","ARM-specific type containing two `int16x4_t` vectors."],["int16x4x3_t","ARM-specific type containing three `int16x4_t` vectors."],["int16x4x4_t","ARM-specific type containing four `int16x4_t` vectors."],["int16x8_t","ARM-specific 128-bit wide vector of eight packed `i16`."],["int16x8x2_t","ARM-specific type containing two `int16x8_t` vectors."],["int16x8x3_t","ARM-specific type containing three `int16x8_t` vectors."],["int16x8x4_t","ARM-specific type containing four `int16x8_t` vectors."],["int32x2_t","ARM-specific 64-bit wide vector of two packed `i32`."],["int32x2x2_t","ARM-specific type containing two `int32x2_t` vectors."],["int32x2x3_t","ARM-specific type containing three `int32x2_t` vectors."],["int32x2x4_t","ARM-specific type containing four `int32x2_t` vectors."],["int32x4_t","ARM-specific 128-bit wide vector of four packed `i32`."],["int32x4x2_t","ARM-specific type containing two `int32x4_t` vectors."],["int32x4x3_t","ARM-specific type containing three `int32x4_t` vectors."],["int32x4x4_t","ARM-specific type containing four `int32x4_t` vectors."],["int64x1_t","ARM-specific 64-bit wide vector of one packed `i64`."],["int64x1x2_t","ARM-specific type containing four `int64x1_t` vectors."],["int64x1x3_t","ARM-specific type containing four `int64x1_t` vectors."],["int64x1x4_t","ARM-specific type containing four `int64x1_t` vectors."],["int64x2_t","ARM-specific 128-bit wide vector of two packed `i64`."],["int64x2x2_t","ARM-specific type containing four `int64x2_t` vectors."],["int64x2x3_t","ARM-specific type containing four `int64x2_t` vectors."],["int64x2x4_t","ARM-specific type containing four `int64x2_t` vectors."],["int8x16_t","ARM-specific 128-bit wide vector of sixteen packed `i8`."],["int8x16x2_t","ARM-specific type containing two `int8x16_t` vectors."],["int8x16x3_t","ARM-specific type containing three `int8x16_t` vectors."],["int8x16x4_t","ARM-specific type containing four `int8x16_t` vectors."],["int8x4_t","ARM-specific 32-bit wide vector of four packed `i8`."],["int8x8_t","ARM-specific 64-bit wide vector of eight packed `i8`."],["int8x8x2_t","ARM-specific type containing two `int8x8_t` vectors."],["int8x8x3_t","ARM-specific type containing three `int8x8_t` vectors."],["int8x8x4_t","ARM-specific type containing four `int8x8_t` vectors."],["poly16x4_t","ARM-specific 64-bit wide vector of four packed `p16`."],["poly16x4x2_t","ARM-specific type containing two `poly16x4_t` vectors."],["poly16x4x3_t","ARM-specific type containing three `poly16x4_t` vectors."],["poly16x4x4_t","ARM-specific type containing four `poly16x4_t` vectors."],["poly16x8_t","ARM-specific 128-bit wide vector of eight packed `p16`."],["poly16x8x2_t","ARM-specific type containing two `poly16x8_t` vectors."],["poly16x8x3_t","ARM-specific type containing three `poly16x8_t` vectors."],["poly16x8x4_t","ARM-specific type containing four `poly16x8_t` vectors."],["poly64x1_t","ARM-specific 64-bit wide vector of one packed `p64`."],["poly64x1x2_t","ARM-specific type containing four `poly64x1_t` vectors."],["poly64x1x3_t","ARM-specific type containing four `poly64x1_t` vectors."],["poly64x1x4_t","ARM-specific type containing four `poly64x1_t` vectors."],["poly64x2_t","ARM-specific 128-bit wide vector of two packed `p64`."],["poly64x2x2_t","ARM-specific type containing four `poly64x2_t` vectors."],["poly64x2x3_t","ARM-specific type containing four `poly64x2_t` vectors."],["poly64x2x4_t","ARM-specific type containing four `poly64x2_t` vectors."],["poly8x16_t","ARM-specific 128-bit wide vector of sixteen packed `p8`."],["poly8x16x2_t","ARM-specific type containing two `poly8x16_t` vectors."],["poly8x16x3_t","ARM-specific type containing three `poly8x16_t` vectors."],["poly8x16x4_t","ARM-specific type containing four `poly8x16_t` vectors."],["poly8x8_t","ARM-specific 64-bit wide polynomial vector of eight packed `p8`."],["poly8x8x2_t","ARM-specific type containing two `poly8x8_t` vectors."],["poly8x8x3_t","ARM-specific type containing three `poly8x8_t` vectors."],["poly8x8x4_t","ARM-specific type containing four `poly8x8_t` vectors."],["uint16x2_t","ARM-specific 32-bit wide vector of two packed `u16`."],["uint16x4_t","ARM-specific 64-bit wide vector of four packed `u16`."],["uint16x4x2_t","ARM-specific type containing two `uint16x4_t` vectors."],["uint16x4x3_t","ARM-specific type containing three `uint16x4_t` vectors."],["uint16x4x4_t","ARM-specific type containing four `uint16x4_t` vectors."],["uint16x8_t","ARM-specific 128-bit wide vector of eight packed `u16`."],["uint16x8x2_t","ARM-specific type containing two `uint16x8_t` vectors."],["uint16x8x3_t","ARM-specific type containing three `uint16x8_t` vectors."],["uint16x8x4_t","ARM-specific type containing four `uint16x8_t` vectors."],["uint32x2_t","ARM-specific 64-bit wide vector of two packed `u32`."],["uint32x2x2_t","ARM-specific type containing two `uint32x2_t` vectors."],["uint32x2x3_t","ARM-specific type containing three `uint32x2_t` vectors."],["uint32x2x4_t","ARM-specific type containing four `uint32x2_t` vectors."],["uint32x4_t","ARM-specific 128-bit wide vector of four packed `u32`."],["uint32x4x2_t","ARM-specific type containing two `uint32x4_t` vectors."],["uint32x4x3_t","ARM-specific type containing three `uint32x4_t` vectors."],["uint32x4x4_t","ARM-specific type containing four `uint32x4_t` vectors."],["uint64x1_t","ARM-specific 64-bit wide vector of one packed `u64`."],["uint64x1x2_t","ARM-specific type containing four `uint64x1_t` vectors."],["uint64x1x3_t","ARM-specific type containing four `uint64x1_t` vectors."],["uint64x1x4_t","ARM-specific type containing four `uint64x1_t` vectors."],["uint64x2_t","ARM-specific 128-bit wide vector of two packed `u64`."],["uint64x2x2_t","ARM-specific type containing four `uint64x2_t` vectors."],["uint64x2x3_t","ARM-specific type containing four `uint64x2_t` vectors."],["uint64x2x4_t","ARM-specific type containing four `uint64x2_t` vectors."],["uint8x16_t","ARM-specific 128-bit wide vector of sixteen packed `u8`."],["uint8x16x2_t","ARM-specific type containing two `uint8x16_t` vectors."],["uint8x16x3_t","ARM-specific type containing three `uint8x16_t` vectors."],["uint8x16x4_t","ARM-specific type containing four `uint8x16_t` vectors."],["uint8x4_t","ARM-specific 32-bit wide vector of four packed `u8`."],["uint8x8_t","ARM-specific 64-bit wide vector of eight packed `u8`."],["uint8x8x2_t","ARM-specific type containing two `uint8x8_t` vectors."],["uint8x8x3_t","ARM-specific type containing three `uint8x8_t` vectors."],["uint8x8x4_t","ARM-specific type containing four `uint8x8_t` vectors."]]});